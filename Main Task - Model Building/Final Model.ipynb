{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "NrvxP6cAxlCK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4f44c7e-1ea0-433c-fa8b-1ce2f7946ca9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Loading data...\n",
            "\n",
            "Dataset Information:\n",
            "Shape: (40, 322)\n",
            "Columns: ['Unnamed: 0', 'alpha1', 'alpha2', 'alpha3', 'alpha4', 'alpha5', 'alpha6', 'alpha7', 'alpha8', 'alpha9', 'alpha10', 'alpha11', 'alpha12', 'alpha13', 'alpha14', 'alpha15', 'alpha16', 'alpha17', 'alpha18', 'alpha19', 'alpha20', 'alpha21', 'alpha22', 'alpha23', 'alpha24', 'alpha25', 'alpha26', 'alpha27', 'alpha28', 'alpha29', 'alpha30', 'alpha31', 'alpha32', 'alpha33', 'alpha34', 'alpha35', 'alpha36', 'alpha37', 'alpha38', 'alpha39', 'alpha40', 'alpha41', 'alpha42', 'alpha43', 'alpha44', 'alpha45', 'alpha46', 'alpha47', 'alpha48', 'alpha49', 'alpha50', 'alpha51', 'alpha52', 'alpha53', 'alpha54', 'alpha55', 'alpha56', 'alpha57', 'alpha58', 'alpha59', 'alpha60', 'alpha61', 'alpha62', 'alpha63', 'alpha64', 'beta1', 'beta2', 'beta3', 'beta4', 'beta5', 'beta6', 'beta7', 'beta8', 'beta9', 'beta10', 'beta11', 'beta12', 'beta13', 'beta14', 'beta15', 'beta16', 'beta17', 'beta18', 'beta19', 'beta20', 'beta21', 'beta22', 'beta23', 'beta24', 'beta25', 'beta26', 'beta27', 'beta28', 'beta29', 'beta30', 'beta31', 'beta32', 'beta33', 'beta34', 'beta35', 'beta36', 'beta37', 'beta38', 'beta39', 'beta40', 'beta41', 'beta42', 'beta43', 'beta44', 'beta45', 'beta46', 'beta47', 'beta48', 'beta49', 'beta50', 'beta51', 'beta52', 'beta53', 'beta54', 'beta55', 'beta56', 'beta57', 'beta58', 'beta59', 'beta60', 'beta61', 'beta62', 'beta63', 'beta64', 'delta1', 'delta2', 'delta3', 'delta4', 'delta5', 'delta6', 'delta7', 'delta8', 'delta9', 'delta10', 'delta11', 'delta12', 'delta13', 'delta14', 'delta15', 'delta16', 'delta17', 'delta18', 'delta19', 'delta20', 'delta21', 'delta22', 'delta23', 'delta24', 'delta25', 'delta26', 'delta27', 'delta28', 'delta29', 'delta30', 'delta31', 'delta32', 'delta33', 'delta34', 'delta35', 'delta36', 'delta37', 'delta38', 'delta39', 'delta40', 'delta41', 'delta42', 'delta43', 'delta44', 'delta45', 'delta46', 'delta47', 'delta48', 'delta49', 'delta50', 'delta51', 'delta52', 'delta53', 'delta54', 'delta55', 'delta56', 'delta57', 'delta58', 'delta59', 'delta60', 'delta61', 'delta62', 'delta63', 'delta64', 'theta1', 'theta2', 'theta3', 'theta4', 'theta5', 'theta6', 'theta7', 'theta8', 'theta9', 'theta10', 'theta11', 'theta12', 'theta13', 'theta14', 'theta15', 'theta16', 'theta17', 'theta18', 'theta19', 'theta20', 'theta21', 'theta22', 'theta23', 'theta24', 'theta25', 'theta26', 'theta27', 'theta28', 'theta29', 'theta30', 'theta31', 'theta32', 'theta33', 'theta34', 'theta35', 'theta36', 'theta37', 'theta38', 'theta39', 'theta40', 'theta41', 'theta42', 'theta43', 'theta44', 'theta45', 'theta46', 'theta47', 'theta48', 'theta49', 'theta50', 'theta51', 'theta52', 'theta53', 'theta54', 'theta55', 'theta56', 'theta57', 'theta58', 'theta59', 'theta60', 'theta61', 'theta62', 'theta63', 'theta64', 'gamma1', 'gamma2', 'gamma3', 'gamma4', 'gamma5', 'gamma6', 'gamma7', 'gamma8', 'gamma9', 'gamma10', 'gamma11', 'gamma12', 'gamma13', 'gamma14', 'gamma15', 'gamma16', 'gamma17', 'gamma18', 'gamma19', 'gamma20', 'gamma21', 'gamma22', 'gamma23', 'gamma24', 'gamma25', 'gamma26', 'gamma27', 'gamma28', 'gamma29', 'gamma30', 'gamma31', 'gamma32', 'gamma33', 'gamma34', 'gamma35', 'gamma36', 'gamma37', 'gamma38', 'gamma39', 'gamma40', 'gamma41', 'gamma42', 'gamma43', 'gamma44', 'gamma45', 'gamma46', 'gamma47', 'gamma48', 'gamma49', 'gamma50', 'gamma51', 'gamma52', 'gamma53', 'gamma54', 'gamma55', 'gamma56', 'gamma57', 'gamma58', 'gamma59', 'gamma60', 'gamma61', 'gamma62', 'gamma63', 'gamma64', 'target']\n",
            "\n",
            "Class Distribution:\n",
            "   Class  Count\n",
            "0    1.0     20\n",
            "1    0.0     20\n",
            "\n",
            "Performing feature selection...\n",
            "Top 10 selected features:\n",
            "     Feature_Index  Feature_Score\n",
            "77             168       1.842427\n",
            "87             178       1.831617\n",
            "26              86       1.777211\n",
            "108            214       1.602173\n",
            "63             150       1.461234\n",
            "7               22       1.438680\n",
            "93             191       1.325837\n",
            "54             138       1.296248\n",
            "76             167       1.220348\n",
            "61             146       1.215837\n",
            "Original dataset: 40 samples, Augmented: 720 samples\n",
            "\n",
            "Training fold 1/5\n",
            "Epoch 10/150, Train Loss: 0.1609, Val Loss: 0.1720, Train Acc: 0.9670, Val Acc: 0.9722, Train F1: 0.9675, Val F1: 0.9718, Overfitting ratio: 1.0688\n",
            "Epoch 20/150, Train Loss: 0.0445, Val Loss: 0.0626, Train Acc: 0.9931, Val Acc: 0.9931, Train F1: 0.9928, Val F1: 0.9930, Overfitting ratio: 1.4066\n",
            "Epoch 30/150, Train Loss: 0.0367, Val Loss: 0.0541, Train Acc: 0.9931, Val Acc: 0.9861, Train F1: 0.9924, Val F1: 0.9859, Overfitting ratio: 1.4764\n",
            "Epoch 40/150, Train Loss: 0.0242, Val Loss: 0.0595, Train Acc: 0.9983, Val Acc: 0.9861, Train F1: 0.9983, Val F1: 0.9859, Overfitting ratio: 2.4572\n",
            "Early stopping at epoch 45\n",
            "\n",
            "Fold 1 Results:\n",
            "Accuracy: 0.9931\n",
            "F1 Score: 0.9930\n",
            "Precision: 1.0000\n",
            "Recall: 0.9861\n",
            "MCC: 0.9862\n",
            "Balanced Accuracy: 0.9931\n",
            "Cohen's Kappa: 0.9861\n",
            "AUC: 0.9998\n",
            "Confusion Matrix:\n",
            "[[72  0]\n",
            " [ 1 71]]\n",
            "Overfitting ratio (val_loss/train_loss): 1.4209\n",
            "Target ratio should be close to 1.0. Ratio > 1.5 indicates overfitting\n",
            "Overfitting analysis for fold 1: Mild signs of overfitting detected\n",
            "\n",
            "Training fold 2/5\n",
            "Epoch 10/150, Train Loss: 0.1288, Val Loss: 0.1939, Train Acc: 0.9740, Val Acc: 0.9097, Train F1: 0.9734, Val F1: 0.9065, Overfitting ratio: 1.5054\n",
            "Epoch 20/150, Train Loss: 0.0458, Val Loss: 0.0897, Train Acc: 0.9913, Val Acc: 0.9653, Train F1: 0.9912, Val F1: 0.9645, Overfitting ratio: 1.9557\n",
            "Epoch 30/150, Train Loss: 0.0282, Val Loss: 0.0978, Train Acc: 0.9948, Val Acc: 0.9653, Train F1: 0.9946, Val F1: 0.9645, Overfitting ratio: 3.4662\n",
            "Epoch 40/150, Train Loss: 0.0196, Val Loss: 0.0931, Train Acc: 0.9983, Val Acc: 0.9653, Train F1: 0.9982, Val F1: 0.9645, Overfitting ratio: 4.7427\n",
            "Early stopping at epoch 40\n",
            "\n",
            "Fold 2 Results:\n",
            "Accuracy: 0.9653\n",
            "F1 Score: 0.9650\n",
            "Precision: 0.9718\n",
            "Recall: 0.9583\n",
            "MCC: 0.9306\n",
            "Balanced Accuracy: 0.9653\n",
            "Cohen's Kappa: 0.9306\n",
            "AUC: 0.9975\n",
            "Confusion Matrix:\n",
            "[[70  2]\n",
            " [ 3 69]]\n",
            "Overfitting ratio (val_loss/train_loss): 2.6672\n",
            "Target ratio should be close to 1.0. Ratio > 1.5 indicates overfitting\n",
            "Overfitting analysis for fold 2: Mild signs of overfitting detected\n",
            "\n",
            "Training fold 3/5\n",
            "Epoch 10/150, Train Loss: 0.1565, Val Loss: 0.1529, Train Acc: 0.9740, Val Acc: 0.9653, Train F1: 0.9721, Val F1: 0.9664, Overfitting ratio: 0.9767\n",
            "Epoch 20/150, Train Loss: 0.0405, Val Loss: 0.0445, Train Acc: 0.9948, Val Acc: 0.9931, Train F1: 0.9948, Val F1: 0.9930, Overfitting ratio: 1.0995\n",
            "Epoch 30/150, Train Loss: 0.0280, Val Loss: 0.0402, Train Acc: 0.9965, Val Acc: 0.9861, Train F1: 0.9965, Val F1: 0.9861, Overfitting ratio: 1.4331\n",
            "Epoch 40/150, Train Loss: 0.0240, Val Loss: 0.0349, Train Acc: 1.0000, Val Acc: 0.9931, Train F1: 1.0000, Val F1: 0.9930, Overfitting ratio: 1.4508\n",
            "Epoch 50/150, Train Loss: 0.0116, Val Loss: 0.0182, Train Acc: 0.9965, Val Acc: 1.0000, Train F1: 0.9964, Val F1: 1.0000, Overfitting ratio: 1.5745\n",
            "Epoch 60/150, Train Loss: 0.0164, Val Loss: 0.0127, Train Acc: 0.9931, Val Acc: 1.0000, Train F1: 0.9932, Val F1: 1.0000, Overfitting ratio: 0.7743\n",
            "Early stopping at epoch 67\n",
            "\n",
            "Fold 3 Results:\n",
            "Accuracy: 1.0000\n",
            "F1 Score: 1.0000\n",
            "Precision: 1.0000\n",
            "Recall: 1.0000\n",
            "MCC: 1.0000\n",
            "Balanced Accuracy: 1.0000\n",
            "Cohen's Kappa: 1.0000\n",
            "AUC: 1.0000\n",
            "Confusion Matrix:\n",
            "[[72  0]\n",
            " [ 0 72]]\n",
            "Overfitting ratio (val_loss/train_loss): 0.7676\n",
            "Target ratio should be close to 1.0. Ratio > 1.5 indicates overfitting\n",
            "Overfitting analysis for fold 3: No signs of overfitting detected\n",
            "\n",
            "Training fold 4/5\n",
            "Epoch 10/150, Train Loss: 0.1551, Val Loss: 0.1938, Train Acc: 0.9670, Val Acc: 0.9375, Train F1: 0.9672, Val F1: 0.9388, Overfitting ratio: 1.2496\n",
            "Epoch 20/150, Train Loss: 0.0385, Val Loss: 0.0948, Train Acc: 0.9931, Val Acc: 0.9722, Train F1: 0.9933, Val F1: 0.9718, Overfitting ratio: 2.4599\n",
            "Epoch 30/150, Train Loss: 0.0277, Val Loss: 0.0825, Train Acc: 1.0000, Val Acc: 0.9792, Train F1: 1.0000, Val F1: 0.9787, Overfitting ratio: 2.9725\n",
            "Epoch 40/150, Train Loss: 0.0277, Val Loss: 0.0771, Train Acc: 1.0000, Val Acc: 0.9792, Train F1: 1.0000, Val F1: 0.9787, Overfitting ratio: 2.7797\n",
            "Epoch 50/150, Train Loss: 0.0090, Val Loss: 0.0561, Train Acc: 1.0000, Val Acc: 0.9792, Train F1: 1.0000, Val F1: 0.9790, Overfitting ratio: 6.2317\n",
            "Epoch 60/150, Train Loss: 0.0050, Val Loss: 0.0568, Train Acc: 1.0000, Val Acc: 0.9931, Train F1: 1.0000, Val F1: 0.9930, Overfitting ratio: 11.2521\n",
            "Early stopping at epoch 67\n",
            "\n",
            "Fold 4 Results:\n",
            "Accuracy: 0.9861\n",
            "F1 Score: 0.9859\n",
            "Precision: 1.0000\n",
            "Recall: 0.9722\n",
            "MCC: 0.9726\n",
            "Balanced Accuracy: 0.9861\n",
            "Cohen's Kappa: 0.9722\n",
            "AUC: 0.9977\n",
            "Confusion Matrix:\n",
            "[[72  0]\n",
            " [ 2 70]]\n",
            "Overfitting ratio (val_loss/train_loss): 7.3143\n",
            "Target ratio should be close to 1.0. Ratio > 1.5 indicates overfitting\n",
            "Overfitting analysis for fold 4: Mild signs of overfitting detected\n",
            "\n",
            "Training fold 5/5\n",
            "Epoch 10/150, Train Loss: 0.1289, Val Loss: 0.1917, Train Acc: 0.9740, Val Acc: 0.9375, Train F1: 0.9744, Val F1: 0.9343, Overfitting ratio: 1.4873\n",
            "Epoch 20/150, Train Loss: 0.0316, Val Loss: 0.1225, Train Acc: 0.9931, Val Acc: 0.9514, Train F1: 0.9928, Val F1: 0.9510, Overfitting ratio: 3.8730\n",
            "Epoch 30/150, Train Loss: 0.0212, Val Loss: 0.1354, Train Acc: 0.9965, Val Acc: 0.9514, Train F1: 0.9965, Val F1: 0.9504, Overfitting ratio: 6.3984\n",
            "Epoch 40/150, Train Loss: 0.0160, Val Loss: 0.1257, Train Acc: 0.9983, Val Acc: 0.9444, Train F1: 0.9985, Val F1: 0.9437, Overfitting ratio: 7.8355\n",
            "Early stopping at epoch 40\n",
            "\n",
            "Fold 5 Results:\n",
            "Accuracy: 0.9583\n",
            "F1 Score: 0.9577\n",
            "Precision: 0.9714\n",
            "Recall: 0.9444\n",
            "MCC: 0.9170\n",
            "Balanced Accuracy: 0.9583\n",
            "Cohen's Kappa: 0.9167\n",
            "AUC: 0.9900\n",
            "Confusion Matrix:\n",
            "[[70  2]\n",
            " [ 4 68]]\n",
            "Overfitting ratio (val_loss/train_loss): 3.8954\n",
            "Target ratio should be close to 1.0. Ratio > 1.5 indicates overfitting\n",
            "Overfitting analysis for fold 5: Moderate overfitting detected\n",
            "\n",
            "Overall Results:\n",
            "Mean Accuracy: 0.9806 ± 0.0180\n",
            "Mean F1 Score: 0.9803 ± 0.0182\n",
            "Mean AUC: 0.9970 ± 0.0041\n",
            "\n",
            "Overfitting Status Counts:\n",
            "Mild signs of overfitting detected: 3\n",
            "No signs of overfitting detected: 1\n",
            "Moderate overfitting detected: 1\n",
            "\n",
            "Performing learning curve analysis...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning:\n",
            "\n",
            "Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
            "\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:2524: UserWarning:\n",
            "\n",
            "y_pred contains classes not in y_true\n",
            "\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning:\n",
            "\n",
            "Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning:\n",
            "\n",
            "Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning:\n",
            "\n",
            "Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Creating ensemble of PyTorch models...\n",
            "\n",
            "Ensemble Model Results:\n",
            "Accuracy: 1.0000\n",
            "F1 Score: 1.0000\n",
            "Precision: 1.0000\n",
            "Recall: 1.0000\n",
            "MCC: 1.0000\n",
            "Balanced Accuracy: 1.0000\n",
            "Cohen's Kappa: 1.0000\n",
            "AUC: 1.0000\n",
            "Confusion Matrix:\n",
            "[[20  0]\n",
            " [ 0 20]]\n",
            "\n",
            "Overfitting Analysis Summary:\n",
            "Fold 1: Mild signs of overfitting detected\n",
            "Fold 2: Mild signs of overfitting detected\n",
            "Fold 3: No signs of overfitting detected\n",
            "Fold 4: Mild signs of overfitting detected\n",
            "Fold 5: Moderate overfitting detected\n",
            "\n",
            "Final Generalization Gap (Train Acc - Val Acc): 0.0198\n",
            "No significant signs of overfitting detected based on the accuracy gap\n",
            "Final Loss Gap (Val Loss - Train Loss): 0.0533\n",
            "Average Overfitting Ratio (Val Loss / Train Loss): 4.2750\n",
            "Warning: Average overfitting ratio above 1.5 indicates overfitting\n",
            "\n",
            "Final Results Summary:\n",
            "              Metric  Cross-Validation (Mean)  Cross-Validation (Std)  \\\n",
            "0           Accuracy                 0.980556                0.017975   \n",
            "1           F1 Score                 0.980341                0.018185   \n",
            "2          Precision                 0.988652                0.015540   \n",
            "3             Recall                 0.972222                0.021960   \n",
            "4                MCC                 0.961294                0.035868   \n",
            "5  Balanced Accuracy                 0.980556                0.017975   \n",
            "6      Cohen's Kappa                 0.961111                0.035950   \n",
            "7                AUC                 0.996991                0.004093   \n",
            "\n",
            "   Ensemble  \n",
            "0       1.0  \n",
            "1       1.0  \n",
            "2       1.0  \n",
            "3       1.0  \n",
            "4       1.0  \n",
            "5       1.0  \n",
            "6       1.0  \n",
            "7       1.0  \n",
            "\n",
            "Total execution time: 1.72 minutes\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import (accuracy_score, f1_score, confusion_matrix,\n",
        "                            precision_score, recall_score, roc_curve, auc,\n",
        "                            matthews_corrcoef, balanced_accuracy_score, cohen_kappa_score)\n",
        "from sklearn.feature_selection import SelectKBest, f_classif\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm import tqdm\n",
        "from datetime import datetime\n",
        "import time\n",
        "import os\n",
        "\n",
        "os.makedirs('visualization', exist_ok=True)\n",
        "os.makedirs('models', exist_ok=True)\n",
        "\n",
        "SEED = 42\n",
        "torch.manual_seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(SEED)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "BATCH_SIZE = 16\n",
        "EPOCHS = 150\n",
        "LEARNING_RATE = 0.0005\n",
        "WEIGHT_DECAY = 5e-4\n",
        "SELECTED_FEATURES = 150\n",
        "NUM_BANDS = 5\n",
        "\n",
        "CHANNEL_COORDS = {\n",
        "    1: (0.75, 0.15), 2: (0.65, 0.2), 3: (0.55, 0.25), 4: (0.45, 0.3), 5: (0.35, 0.2),\n",
        "    6: (0.25, 0.2), 7: (0.4, 0.35), 8: (0.5, 0.15), 9: (0.45, 0.25), 10: (0.35, 0.15),\n",
        "    11: (0.3, 0.25), 12: (0.4, 0.3), 13: (0.25, 0.25), 14: (0.35, 0.3), 15: (0.3, 0.35),\n",
        "    16: (0.35, 0.4), 17: (0.2, 0.2), 18: (0.25, 0.3), 19: (0.2, 0.3), 20: (0.3, 0.4),\n",
        "    21: (0.4, 0.45), 22: (0.25, 0.4), 23: (0.15, 0.35), 24: (0.2, 0.4), 25: (0.25, 0.45),\n",
        "    26: (0.3, 0.5), 27: (0.2, 0.5), 28: (0.25, 0.55), 29: (0.15, 0.55), 30: (0.2, 0.6),\n",
        "    31: (0.35, 0.55), 32: (0.15, 0.65), 33: (0.3, 0.65), 34: (0.45, 0.35), 35: (0.4, 0.7),\n",
        "    36: (0.35, 0.6), 37: (0.4, 0.65), 38: (0.35, 0.65), 39: (0.45, 0.7), 40: (0.45, 0.6),\n",
        "    41: (0.55, 0.45), 42: (0.55, 0.6), 43: (0.6, 0.65), 44: (0.65, 0.65), 45: (0.6, 0.55),\n",
        "    46: (0.55, 0.5), 47: (0.65, 0.6), 48: (0.55, 0.55), 49: (0.6, 0.4), 50: (0.6, 0.35),\n",
        "    51: (0.55, 0.4), 52: (0.65, 0.45), 53: (0.65, 0.35), 54: (0.55, 0.35), 55: (0.7, 0.4),\n",
        "    56: (0.65, 0.3), 57: (0.6, 0.3), 58: (0.7, 0.25), 59: (0.65, 0.25), 60: (0.55, 0.3),\n",
        "    61: (0.75, 0.2), 62: (0.5, 0.1), 63: (0.45, 0.05), 64: (0.15, 0.2)\n",
        "}\n",
        "\n",
        "class MetricsTracker:\n",
        "    def __init__(self, fold, epochs):\n",
        "        self.fold = fold\n",
        "        self.epochs = epochs\n",
        "        self.train_losses = []\n",
        "        self.val_losses = []\n",
        "        self.train_accs = []\n",
        "        self.val_accs = []\n",
        "        self.train_f1s = []\n",
        "        self.val_f1s = []\n",
        "        self.train_precisions = []\n",
        "        self.val_precisions = []\n",
        "        self.train_recalls = []\n",
        "        self.val_recalls = []\n",
        "        self.epoch_nums = []\n",
        "        self.overfitting_ratios = []\n",
        "        self.mccs = []\n",
        "        self.balanced_accs = []\n",
        "        self.kappas = []\n",
        "\n",
        "    def update(self, epoch, train_metrics, val_metrics):\n",
        "        self.epoch_nums.append(epoch)\n",
        "        self.train_losses.append(train_metrics['loss'])\n",
        "        self.val_losses.append(val_metrics['loss'])\n",
        "        self.train_accs.append(train_metrics['accuracy'])\n",
        "        self.val_accs.append(val_metrics['accuracy'])\n",
        "        self.train_f1s.append(train_metrics['f1'])\n",
        "        self.val_f1s.append(val_metrics['f1'])\n",
        "        self.train_precisions.append(train_metrics['precision'])\n",
        "        self.val_precisions.append(val_metrics['precision'])\n",
        "        self.train_recalls.append(train_metrics['recall'])\n",
        "        self.val_recalls.append(val_metrics['recall'])\n",
        "        self.mccs.append(val_metrics['mcc'])\n",
        "        self.balanced_accs.append(val_metrics['balanced_acc'])\n",
        "        self.kappas.append(val_metrics['kappa'])\n",
        "\n",
        "        ratio = val_metrics['loss'] / train_metrics['loss'] if train_metrics['loss'] > 0 else 1.0\n",
        "        self.overfitting_ratios.append(ratio)\n",
        "\n",
        "    def plot_metrics(self):\n",
        "        fig, axes = plt.subplots(3, 2, figsize=(16, 18))\n",
        "\n",
        "        axes[0, 0].plot(self.epoch_nums, self.train_losses, 'b-', label='Training Loss')\n",
        "        axes[0, 0].plot(self.epoch_nums, self.val_losses, 'r-', label='Validation Loss')\n",
        "        axes[0, 0].set_xlabel('Epoch')\n",
        "        axes[0, 0].set_ylabel('Loss')\n",
        "        axes[0, 0].set_title(f'Fold {self.fold}: Loss Curves')\n",
        "        axes[0, 0].legend()\n",
        "        axes[0, 0].grid(True)\n",
        "\n",
        "        axes[0, 1].plot(self.epoch_nums, self.train_accs, 'b-', label='Training Accuracy')\n",
        "        axes[0, 1].plot(self.epoch_nums, self.val_accs, 'r-', label='Validation Accuracy')\n",
        "        axes[0, 1].set_xlabel('Epoch')\n",
        "        axes[0, 1].set_ylabel('Accuracy')\n",
        "        axes[0, 1].set_title(f'Fold {self.fold}: Accuracy Curves')\n",
        "        axes[0, 1].legend()\n",
        "        axes[0, 1].grid(True)\n",
        "\n",
        "        axes[1, 0].plot(self.epoch_nums, self.train_f1s, 'b-', label='Training F1')\n",
        "        axes[1, 0].plot(self.epoch_nums, self.val_f1s, 'r-', label='Validation F1')\n",
        "        axes[1, 0].set_xlabel('Epoch')\n",
        "        axes[1, 0].set_ylabel('F1 Score')\n",
        "        axes[1, 0].set_title(f'Fold {self.fold}: F1 Score Curves')\n",
        "        axes[1, 0].legend()\n",
        "        axes[1, 0].grid(True)\n",
        "\n",
        "        axes[1, 1].plot(self.epoch_nums, self.train_precisions, 'b-', label='Training Precision')\n",
        "        axes[1, 1].plot(self.epoch_nums, self.val_precisions, 'r-', label='Validation Precision')\n",
        "        axes[1, 1].plot(self.epoch_nums, self.train_recalls, 'g-', label='Training Recall')\n",
        "        axes[1, 1].plot(self.epoch_nums, self.val_recalls, 'y-', label='Validation Recall')\n",
        "        axes[1, 1].set_xlabel('Epoch')\n",
        "        axes[1, 1].set_ylabel('Score')\n",
        "        axes[1, 1].set_title(f'Fold {self.fold}: Precision & Recall')\n",
        "        axes[1, 1].legend()\n",
        "        axes[1, 1].grid(True)\n",
        "\n",
        "        axes[2, 0].plot(self.epoch_nums, self.overfitting_ratios, 'b-', label='Overfitting Ratio')\n",
        "        axes[2, 0].axhline(y=1.0, color='r', linestyle='--', label='Ideal Ratio = 1.0')\n",
        "        axes[2, 0].axhline(y=1.5, color='orange', linestyle='--', label='Warning Threshold = 1.5')\n",
        "        axes[2, 0].set_xlabel('Epoch')\n",
        "        axes[2, 0].set_ylabel('Ratio')\n",
        "        axes[2, 0].set_title(f'Fold {self.fold}: Overfitting Ratio')\n",
        "        axes[2, 0].legend()\n",
        "        axes[2, 0].grid(True)\n",
        "\n",
        "        axes[2, 1].plot(self.epoch_nums, self.mccs, 'b-', label='MCC')\n",
        "        axes[2, 1].plot(self.epoch_nums, self.balanced_accs, 'r-', label='Balanced Accuracy')\n",
        "        axes[2, 1].plot(self.epoch_nums, self.kappas, 'g-', label='Cohen\\'s Kappa')\n",
        "        axes[2, 1].set_xlabel('Epoch')\n",
        "        axes[2, 1].set_ylabel('Score')\n",
        "        axes[2, 1].set_title(f'Fold {self.fold}: Advanced Metrics')\n",
        "        axes[2, 1].legend()\n",
        "        axes[2, 1].grid(True)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(f'visualization/fold_{self.fold}_metrics.png')\n",
        "        plt.close()\n",
        "\n",
        "    def analyze_overfitting(self):\n",
        "        final_ratio = self.overfitting_ratios[-1] if self.overfitting_ratios else 1.0\n",
        "        avg_final_train_acc = np.mean(self.train_accs[-5:]) if len(self.train_accs) >= 5 else np.mean(self.train_accs)\n",
        "        avg_final_val_acc = np.mean(self.val_accs[-5:]) if len(self.val_accs) >= 5 else np.mean(self.val_accs)\n",
        "        generalization_gap = avg_final_train_acc - avg_final_val_acc\n",
        "\n",
        "        overfitting_score = 0\n",
        "        if final_ratio > 1.5:\n",
        "            overfitting_score += 1\n",
        "        if generalization_gap > 0.05:\n",
        "            overfitting_score += 1\n",
        "        if avg_final_train_acc > 0.98 and avg_final_val_acc < 0.95:\n",
        "            overfitting_score += 1\n",
        "\n",
        "        if overfitting_score == 0:\n",
        "            return \"No signs of overfitting detected\"\n",
        "        elif overfitting_score == 1:\n",
        "            return \"Mild signs of overfitting detected\"\n",
        "        elif overfitting_score == 2:\n",
        "            return \"Moderate overfitting detected\"\n",
        "        else:\n",
        "            return \"Severe overfitting detected\"\n",
        "\n",
        "def calculate_metrics(model, data_loader, criterion, device):\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_targets = []\n",
        "    all_probs = []\n",
        "    total_loss = 0.0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in data_loader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            probs = outputs.cpu().numpy()\n",
        "            preds = (outputs > 0.5).float()\n",
        "\n",
        "            all_probs.extend(probs)\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_targets.extend(targets.cpu().numpy())\n",
        "\n",
        "    avg_loss = total_loss / len(data_loader)\n",
        "    accuracy = accuracy_score(all_targets, all_preds)\n",
        "    f1 = f1_score(all_targets, all_preds)\n",
        "    precision = precision_score(all_targets, all_preds)\n",
        "    recall = recall_score(all_targets, all_preds)\n",
        "    mcc = matthews_corrcoef(all_targets, all_preds)\n",
        "    balanced_acc = balanced_accuracy_score(all_targets, all_preds)\n",
        "    kappa = cohen_kappa_score(all_targets, all_preds)\n",
        "\n",
        "    metrics = {\n",
        "        'loss': avg_loss,\n",
        "        'accuracy': accuracy,\n",
        "        'f1': f1,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'mcc': mcc,\n",
        "        'balanced_acc': balanced_acc,\n",
        "        'kappa': kappa,\n",
        "        'predictions': all_preds,\n",
        "        'targets': all_targets,\n",
        "        'probabilities': all_probs\n",
        "    }\n",
        "\n",
        "    return metrics\n",
        "\n",
        "def plot_cross_val_metrics(fold_monitors):\n",
        "    max_epoch = max([max(monitor.epoch_nums) for monitor in fold_monitors]) if fold_monitors else 0\n",
        "    valid_epochs = []\n",
        "    metrics_avg = {\n",
        "        'train_loss': [], 'val_loss': [],\n",
        "        'train_acc': [], 'val_acc': [],\n",
        "        'train_f1': [], 'val_f1': [],\n",
        "        'train_precision': [], 'val_precision': [],\n",
        "        'train_recall': [], 'val_recall': [],\n",
        "        'mcc': [], 'balanced_acc': [], 'kappa': [],\n",
        "        'overfitting_ratio': []\n",
        "    }\n",
        "\n",
        "    counts = np.zeros(max_epoch)\n",
        "    for i in range(1, max_epoch + 1):\n",
        "        count = 0\n",
        "        temp_metrics = {k: 0 for k in metrics_avg.keys()}\n",
        "\n",
        "        for monitor in fold_monitors:\n",
        "            if i in monitor.epoch_nums:\n",
        "                idx = monitor.epoch_nums.index(i)\n",
        "                temp_metrics['train_loss'] += monitor.train_losses[idx]\n",
        "                temp_metrics['val_loss'] += monitor.val_losses[idx]\n",
        "                temp_metrics['train_acc'] += monitor.train_accs[idx]\n",
        "                temp_metrics['val_acc'] += monitor.val_accs[idx]\n",
        "                temp_metrics['train_f1'] += monitor.train_f1s[idx]\n",
        "                temp_metrics['val_f1'] += monitor.val_f1s[idx]\n",
        "                temp_metrics['train_precision'] += monitor.train_precisions[idx]\n",
        "                temp_metrics['val_precision'] += monitor.val_precisions[idx]\n",
        "                temp_metrics['train_recall'] += monitor.train_recalls[idx]\n",
        "                temp_metrics['val_recall'] += monitor.val_recalls[idx]\n",
        "                temp_metrics['mcc'] += monitor.mccs[idx]\n",
        "                temp_metrics['balanced_acc'] += monitor.balanced_accs[idx]\n",
        "                temp_metrics['kappa'] += monitor.kappas[idx]\n",
        "                temp_metrics['overfitting_ratio'] += monitor.overfitting_ratios[idx]\n",
        "                count += 1\n",
        "\n",
        "        if count > 0:\n",
        "            valid_epochs.append(i)\n",
        "            for k in metrics_avg.keys():\n",
        "                metrics_avg[k].append(temp_metrics[k] / count)\n",
        "            counts[i-1] = count\n",
        "\n",
        "    if valid_epochs:\n",
        "        fig, axes = plt.subplots(3, 2, figsize=(16, 18))\n",
        "\n",
        "        axes[0, 0].plot(valid_epochs, metrics_avg['train_loss'], 'b-', label='Avg Training Loss')\n",
        "        axes[0, 0].plot(valid_epochs, metrics_avg['val_loss'], 'r-', label='Avg Validation Loss')\n",
        "        axes[0, 0].set_xlabel('Epoch')\n",
        "        axes[0, 0].set_ylabel('Loss')\n",
        "        axes[0, 0].set_title('Average Loss Across Folds')\n",
        "        axes[0, 0].legend()\n",
        "        axes[0, 0].grid(True)\n",
        "\n",
        "        axes[0, 1].plot(valid_epochs, metrics_avg['train_acc'], 'b-', label='Avg Training Accuracy')\n",
        "        axes[0, 1].plot(valid_epochs, metrics_avg['val_acc'], 'r-', label='Avg Validation Accuracy')\n",
        "        axes[0, 1].set_xlabel('Epoch')\n",
        "        axes[0, 1].set_ylabel('Accuracy')\n",
        "        axes[0, 1].set_title('Average Accuracy Across Folds')\n",
        "        axes[0, 1].legend()\n",
        "        axes[0, 1].grid(True)\n",
        "\n",
        "        axes[1, 0].plot(valid_epochs, metrics_avg['train_f1'], 'b-', label='Avg Training F1')\n",
        "        axes[1, 0].plot(valid_epochs, metrics_avg['val_f1'], 'r-', label='Avg Validation F1')\n",
        "        axes[1, 0].set_xlabel('Epoch')\n",
        "        axes[1, 0].set_ylabel('F1 Score')\n",
        "        axes[1, 0].set_title('Average F1 Score Across Folds')\n",
        "        axes[1, 0].legend()\n",
        "        axes[1, 0].grid(True)\n",
        "\n",
        "        axes[1, 1].plot(valid_epochs, metrics_avg['train_precision'], 'b-', label='Avg Training Precision')\n",
        "        axes[1, 1].plot(valid_epochs, metrics_avg['val_precision'], 'r-', label='Avg Validation Precision')\n",
        "        axes[1, 1].plot(valid_epochs, metrics_avg['train_recall'], 'g-', label='Avg Training Recall')\n",
        "        axes[1, 1].plot(valid_epochs, metrics_avg['val_recall'], 'y-', label='Avg Validation Recall')\n",
        "        axes[1, 1].set_xlabel('Epoch')\n",
        "        axes[1, 1].set_ylabel('Score')\n",
        "        axes[1, 1].set_title('Average Precision & Recall Across Folds')\n",
        "        axes[1, 1].legend()\n",
        "        axes[1, 1].grid(True)\n",
        "\n",
        "        axes[2, 0].plot(valid_epochs, metrics_avg['overfitting_ratio'], 'b-', label='Avg Overfitting Ratio')\n",
        "        axes[2, 0].axhline(y=1.0, color='r', linestyle='--', label='Ideal Ratio = 1.0')\n",
        "        axes[2, 0].axhline(y=1.5, color='orange', linestyle='--', label='Warning Threshold = 1.5')\n",
        "        axes[2, 0].set_xlabel('Epoch')\n",
        "        axes[2, 0].set_ylabel('Ratio')\n",
        "        axes[2, 0].set_title('Average Overfitting Ratio Across Folds')\n",
        "        axes[2, 0].legend()\n",
        "        axes[2, 0].grid(True)\n",
        "\n",
        "        axes[2, 1].plot(valid_epochs, metrics_avg['mcc'], 'b-', label='Avg MCC')\n",
        "        axes[2, 1].plot(valid_epochs, metrics_avg['balanced_acc'], 'r-', label='Avg Balanced Accuracy')\n",
        "        axes[2, 1].plot(valid_epochs, metrics_avg['kappa'], 'g-', label='Avg Cohen\\'s Kappa')\n",
        "        axes[2, 1].set_xlabel('Epoch')\n",
        "        axes[2, 1].set_ylabel('Score')\n",
        "        axes[2, 1].set_title('Average Advanced Metrics Across Folds')\n",
        "        axes[2, 1].legend()\n",
        "        axes[2, 1].grid(True)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig('visualization/cross_validation_metrics.png')\n",
        "        plt.close()\n",
        "\n",
        "def plot_confusion_matrices(fold_cms, final_cm, class_names=['Negative', 'Positive']):\n",
        "    n_folds = len(fold_cms)\n",
        "    fig, axes = plt.subplots(1, n_folds + 1, figsize=(5 * (n_folds + 1), 5))\n",
        "\n",
        "    for i, cm in enumerate(fold_cms):\n",
        "        cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        sns.heatmap(cm_normalized, annot=cm, fmt=\"d\", cmap=\"Blues\", ax=axes[i],\n",
        "                   xticklabels=class_names, yticklabels=class_names, annot_kws={\"size\": 16})\n",
        "        axes[i].set_title(f'Fold {i+1} Confusion Matrix')\n",
        "        axes[i].set_xlabel('Predicted Label')\n",
        "        axes[i].set_ylabel('True Label')\n",
        "\n",
        "    final_cm_normalized = final_cm.astype('float') / final_cm.sum(axis=1)[:, np.newaxis]\n",
        "    sns.heatmap(final_cm_normalized, annot=final_cm, fmt=\"d\", cmap=\"Blues\", ax=axes[-1],\n",
        "               xticklabels=class_names, yticklabels=class_names, annot_kws={\"size\": 16})\n",
        "    axes[-1].set_title('Ensemble Model Confusion Matrix')\n",
        "    axes[-1].set_xlabel('Predicted Label')\n",
        "    axes[-1].set_ylabel('True Label')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('visualization/confusion_matrices.png')\n",
        "    plt.close()\n",
        "\n",
        "def visualize_feature_importance(X, selected_indices, feature_scores, band_names=['Alpha', 'Beta', 'Delta', 'Theta', 'Gamma']):\n",
        "    num_features = len(feature_scores)\n",
        "\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    plt.bar(range(num_features), feature_scores)\n",
        "    plt.xlabel('Feature Index')\n",
        "    plt.ylabel('Importance Score')\n",
        "    plt.title('Feature Importance (All Features)')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('visualization/feature_importance_all.png')\n",
        "    plt.close()\n",
        "\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    plt.bar(range(len(selected_indices)), feature_scores[selected_indices])\n",
        "    plt.xlabel('Selected Feature Index')\n",
        "    plt.ylabel('Importance Score')\n",
        "    plt.title('Feature Importance (Selected Features)')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('visualization/feature_importance_selected.png')\n",
        "    plt.close()\n",
        "\n",
        "    selected_features_df = pd.DataFrame({\n",
        "        'Feature_Index': selected_indices,\n",
        "        'Feature_Score': feature_scores[selected_indices]\n",
        "    })\n",
        "\n",
        "    selected_features_df = selected_features_df.sort_values('Feature_Score', ascending=False)\n",
        "\n",
        "    if len(selected_indices) >= NUM_BANDS:\n",
        "        band_indices = np.array([i % NUM_BANDS for i in range(len(selected_indices))])\n",
        "\n",
        "        band_importance = np.zeros(NUM_BANDS)\n",
        "        for i in range(NUM_BANDS):\n",
        "            band_mask = (band_indices == i)\n",
        "            if np.any(band_mask):\n",
        "                band_importance[i] = np.mean(feature_scores[selected_indices][band_mask])\n",
        "\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        plt.bar(band_names, band_importance)\n",
        "        plt.xlabel('Frequency Band')\n",
        "        plt.ylabel('Average Importance')\n",
        "        plt.title('Frequency Band Importance')\n",
        "        plt.tight_layout()\n",
        "        plt.savefig('visualization/band_importance.png')\n",
        "        plt.close()\n",
        "\n",
        "    return selected_features_df\n",
        "\n",
        "def visualize_dataset(X, y, feature_names=None):\n",
        "    n_samples, n_features = X.shape\n",
        "\n",
        "    data_stats = pd.DataFrame({\n",
        "        'Feature': feature_names if feature_names else [f'Feature_{i}' for i in range(n_features)],\n",
        "        'Mean': np.mean(X, axis=0),\n",
        "        'Std': np.std(X, axis=0),\n",
        "        'Min': np.min(X, axis=0),\n",
        "        'Max': np.max(X, axis=0)\n",
        "    })\n",
        "\n",
        "    class_distribution = pd.Series(y).value_counts().reset_index()\n",
        "    class_distribution.columns = ['Class', 'Count']\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.countplot(x=y)\n",
        "    plt.title('Class Distribution')\n",
        "    plt.xlabel('Class')\n",
        "    plt.ylabel('Count')\n",
        "    plt.savefig('visualization/class_distribution.png')\n",
        "    plt.close()\n",
        "\n",
        "    if n_features > 1:\n",
        "        pca = PCA(n_components=2)\n",
        "        pca_result = pca.fit_transform(X)\n",
        "\n",
        "        plt.figure(figsize=(10, 8))\n",
        "        scatter = plt.scatter(pca_result[:, 0], pca_result[:, 1], c=y, cmap='viridis', alpha=0.8)\n",
        "        plt.colorbar(scatter)\n",
        "        plt.title('PCA Visualization of Dataset')\n",
        "        plt.xlabel('Principal Component 1')\n",
        "        plt.ylabel('Principal Component 2')\n",
        "        plt.savefig('visualization/pca_visualization.png')\n",
        "        plt.close()\n",
        "\n",
        "        tsne = TSNE(n_components=2, random_state=SEED)\n",
        "        tsne_result = tsne.fit_transform(X)\n",
        "\n",
        "        plt.figure(figsize=(10, 8))\n",
        "        scatter = plt.scatter(tsne_result[:, 0], tsne_result[:, 1], c=y, cmap='viridis', alpha=0.8)\n",
        "        plt.colorbar(scatter)\n",
        "        plt.title('t-SNE Visualization of Dataset')\n",
        "        plt.xlabel('t-SNE Component 1')\n",
        "        plt.ylabel('t-SNE Component 2')\n",
        "        plt.savefig('visualization/tsne_visualization.png')\n",
        "        plt.close()\n",
        "\n",
        "    return data_stats, class_distribution\n",
        "\n",
        "def plot_learning_curve(train_sizes, train_scores, val_scores):\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(train_sizes, train_scores, 'o-', color='blue', label='Training Accuracy')\n",
        "    plt.plot(train_sizes, val_scores, 'o-', color='red', label='Validation Accuracy')\n",
        "    plt.xlabel('Training Set Size')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.title('Learning Curve: Accuracy vs Training Set Size')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('visualization/learning_curve.png')\n",
        "    plt.close()\n",
        "\n",
        "def plot_roc_curve(y_true, y_prob, fold=None):\n",
        "    fpr, tpr, _ = roc_curve(y_true, y_prob)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "\n",
        "    plt.figure(figsize=(8, 8))\n",
        "    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.3f})')\n",
        "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title(f'Receiver Operating Characteristic{\" - Fold \" + str(fold) if fold else \"\"}')\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.savefig(f'visualization/roc_curve{\"_fold_\" + str(fold) if fold else \"\"}.png')\n",
        "    plt.close()\n",
        "\n",
        "    return roc_auc\n",
        "\n",
        "def visualize_electrode_map(importance_values=None, title='EEG Electrode Map'):\n",
        "    fig, ax = plt.subplots(figsize=(10, 10))\n",
        "\n",
        "    for ch_idx, (x, y) in CHANNEL_COORDS.items():\n",
        "        x_scaled = x * 100\n",
        "        y_scaled = y * 100\n",
        "\n",
        "        if importance_values is not None and ch_idx-1 < len(importance_values):\n",
        "            importance = importance_values[ch_idx-1]\n",
        "            size = 20 + 100 * importance\n",
        "            color = plt.cm.viridis(importance)\n",
        "        else:\n",
        "            size = 50\n",
        "            color = 'blue'\n",
        "\n",
        "        ax.scatter(x_scaled, y_scaled, s=size, color=color, alpha=0.7)\n",
        "        ax.text(x_scaled, y_scaled, str(ch_idx), ha='center', va='center', fontsize=8, color='white')\n",
        "\n",
        "    if importance_values is not None:\n",
        "        sm = plt.cm.ScalarMappable(cmap=plt.cm.viridis, norm=plt.Normalize(vmin=0, vmax=1))\n",
        "        sm.set_array([])\n",
        "        cbar = plt.colorbar(sm, ax=ax)\n",
        "        cbar.set_label('Importance')\n",
        "\n",
        "    ax.set_xlim(0, 100)\n",
        "    ax.set_ylim(0, 100)\n",
        "    ax.set_title(title)\n",
        "    ax.set_aspect('equal')\n",
        "    ax.set_axis_off()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('visualization/electrode_map.png')\n",
        "    plt.close()\n",
        "\n",
        "class EEGDataset(Dataset):\n",
        "    def __init__(self, features, targets):\n",
        "        self.features = features\n",
        "        self.targets = targets\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.targets)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.features[idx], self.targets[idx]\n",
        "\n",
        "class EEGChannelAttention(nn.Module):\n",
        "    def __init__(self, num_channels):\n",
        "        super(EEGChannelAttention, self).__init__()\n",
        "        self.attention = nn.Sequential(\n",
        "            nn.Linear(num_channels, num_channels // 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(num_channels // 2, num_channels),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        attention_weights = self.attention(x.mean(dim=1))\n",
        "        return x * attention_weights.unsqueeze(1), attention_weights\n",
        "\n",
        "class EEGBandAttention(nn.Module):\n",
        "    def __init__(self, num_bands):\n",
        "        super(EEGBandAttention, self).__init__()\n",
        "        self.attention = nn.Sequential(\n",
        "            nn.Linear(num_bands, num_bands),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(num_bands, num_bands),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        attention_weights = self.attention(x.mean(dim=2))\n",
        "        return x * attention_weights.unsqueeze(2), attention_weights\n",
        "\n",
        "class SpatialGNN(nn.Module):\n",
        "    def __init__(self, num_channels, adjacency_matrix):\n",
        "        super(SpatialGNN, self).__init__()\n",
        "        self.adjacency_matrix = adjacency_matrix.to(device)\n",
        "        self.weight = nn.Parameter(torch.Tensor(num_channels, num_channels))\n",
        "        nn.init.xavier_uniform_(self.weight)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, num_bands, _ = x.shape\n",
        "        output = torch.zeros_like(x)\n",
        "\n",
        "        for b in range(num_bands):\n",
        "            features = x[:, b, :]\n",
        "            neighbor_agg = torch.matmul(self.adjacency_matrix, features.unsqueeze(2))\n",
        "            transformed = torch.matmul(neighbor_agg.squeeze(2), self.weight)\n",
        "            output[:, b, :] = transformed\n",
        "\n",
        "        return output + x\n",
        "\n",
        "class EEGClassifier(nn.Module):\n",
        "    def __init__(self, num_bands, num_channels, adjacency_matrix):\n",
        "        super(EEGClassifier, self).__init__()\n",
        "\n",
        "        self.spatial_gnn = SpatialGNN(num_channels, adjacency_matrix)\n",
        "        self.channel_attention = EEGChannelAttention(num_channels)\n",
        "        self.band_attention = EEGBandAttention(num_bands)\n",
        "\n",
        "        self.feature_extraction = nn.Sequential(\n",
        "            nn.Linear(num_bands * num_channels, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm1d(256),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(256, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm1d(128),\n",
        "            nn.Dropout(0.4)\n",
        "        )\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(128, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm1d(64),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(64, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "        self.attention_weights = {\n",
        "            'channel': None,\n",
        "            'band': None\n",
        "        }\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.spatial_gnn(x)\n",
        "        x, channel_weights = self.channel_attention(x)\n",
        "        x, band_weights = self.band_attention(x)\n",
        "\n",
        "        self.attention_weights['channel'] = channel_weights\n",
        "        self.attention_weights['band'] = band_weights\n",
        "\n",
        "        x = x.reshape(x.size(0), -1)\n",
        "        x = self.feature_extraction(x)\n",
        "        x = self.classifier(x)\n",
        "\n",
        "        return x.squeeze(1)\n",
        "\n",
        "    def get_attention_weights(self):\n",
        "        return self.attention_weights\n",
        "\n",
        "class SimplifiedEEGClassifier(nn.Module):\n",
        "    def __init__(self, num_bands, num_channels, adjacency_matrix):\n",
        "        super(SimplifiedEEGClassifier, self).__init__()\n",
        "\n",
        "        self.spatial_gnn = SpatialGNN(num_channels, adjacency_matrix)\n",
        "        self.channel_attention = EEGChannelAttention(num_channels)\n",
        "\n",
        "        self.feature_extraction = nn.Sequential(\n",
        "            nn.Linear(num_bands * num_channels, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm1d(128),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm1d(64),\n",
        "            nn.Dropout(0.4)\n",
        "        )\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(64, 32),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm1d(32),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(32, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "        self.attention_weights = {\n",
        "            'channel': None\n",
        "        }\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.spatial_gnn(x)\n",
        "        x, channel_weights = self.channel_attention(x)\n",
        "\n",
        "        self.attention_weights['channel'] = channel_weights\n",
        "\n",
        "        x = x.reshape(x.size(0), -1)\n",
        "        x = self.feature_extraction(x)\n",
        "        x = self.classifier(x)\n",
        "\n",
        "        return x.squeeze(1)\n",
        "\n",
        "    def get_attention_weights(self):\n",
        "        return self.attention_weights\n",
        "\n",
        "def create_adjacency_matrix(channel_coords, num_channels_per_band, threshold=0.2):\n",
        "    adj_matrix = torch.eye(num_channels_per_band)\n",
        "\n",
        "    for i in range(num_channels_per_band):\n",
        "        for j in range(num_channels_per_band):\n",
        "            if i != j and abs(i - j) <= 2:\n",
        "                adj_matrix[i, j] = 0.5\n",
        "\n",
        "    row_sum = adj_matrix.sum(dim=1, keepdim=True)\n",
        "    row_sum[row_sum == 0] = 1.0\n",
        "    adj_matrix = adj_matrix / row_sum\n",
        "\n",
        "    return adj_matrix\n",
        "\n",
        "def augment_data(X, y, noise_levels=[0.03, 0.05, 0.07], num_augmentations=3):\n",
        "    X_aug, y_aug = X.copy(), y.copy()\n",
        "\n",
        "    for noise_level in noise_levels:\n",
        "        for _ in range(num_augmentations):\n",
        "            noise = np.random.normal(0, noise_level, X.shape)\n",
        "            X_noisy = X + noise\n",
        "            X_aug = np.vstack((X_aug, X_noisy))\n",
        "            y_aug = np.append(y_aug, y)\n",
        "\n",
        "    for scale in [0.9, 1.1]:\n",
        "        X_scaled = X * scale\n",
        "        X_aug = np.vstack((X_aug, X_scaled))\n",
        "        y_aug = np.append(y_aug, y)\n",
        "\n",
        "    n_features = X.shape[1]\n",
        "    for _ in range(num_augmentations):\n",
        "        X_channel_drop = X.copy()\n",
        "        drop_idx = np.random.choice(n_features, size=int(0.05 * n_features), replace=False)\n",
        "        X_channel_drop[:, drop_idx] = 0\n",
        "        X_aug = np.vstack((X_aug, X_channel_drop))\n",
        "        y_aug = np.append(y_aug, y)\n",
        "\n",
        "    features_per_band = n_features // 5\n",
        "    for _ in range(num_augmentations):\n",
        "        X_perm = X.copy()\n",
        "        for band in range(5):\n",
        "            band_start = band * features_per_band\n",
        "            band_end = (band + 1) * features_per_band\n",
        "            perm_indices = np.random.permutation(features_per_band)[:int(0.2 * features_per_band)]\n",
        "            for idx in perm_indices:\n",
        "                if band_start + idx < n_features:\n",
        "                    source_idx = band_start + idx\n",
        "                    target_idx = band_start + np.random.randint(0, features_per_band)\n",
        "                    if target_idx < n_features:\n",
        "                        X_perm[:, [source_idx, target_idx]] = X_perm[:, [target_idx, source_idx]]\n",
        "        X_aug = np.vstack((X_aug, X_perm))\n",
        "        y_aug = np.append(y_aug, y)\n",
        "\n",
        "    print(f\"Original dataset: {X.shape[0]} samples, Augmented: {X_aug.shape[0]} samples\")\n",
        "    return X_aug, y_aug\n",
        "\n",
        "def reshape_data_to_bands_channels(X, feature_indices, num_bands=5):\n",
        "    n_samples = X.shape[0]\n",
        "    n_selected_features = X.shape[1]\n",
        "    channels_per_band = n_selected_features // num_bands\n",
        "    reshaped = np.zeros((n_samples, num_bands, channels_per_band))\n",
        "\n",
        "    feature_idx = 0\n",
        "    for b in range(num_bands):\n",
        "        for c in range(channels_per_band):\n",
        "            if feature_idx < n_selected_features:\n",
        "                reshaped[:, b, c] = X[:, feature_idx]\n",
        "                feature_idx += 1\n",
        "\n",
        "    return reshaped\n",
        "\n",
        "def create_pytorch_ensemble(X_scaled, y, selected_indices, adjacency_matrix, device, n_splits=5):\n",
        "    print(\"\\nCreating ensemble of PyTorch models...\")\n",
        "    ensemble_preds = np.zeros(len(y))\n",
        "    ensemble_probs = np.zeros(len(y))\n",
        "\n",
        "    X_reshaped = reshape_data_to_bands_channels(X_scaled, selected_indices)\n",
        "    full_dataset = EEGDataset(torch.FloatTensor(X_reshaped), torch.FloatTensor(y))\n",
        "    full_loader = DataLoader(full_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "    for fold in range(n_splits):\n",
        "        model = EEGClassifier(\n",
        "            num_bands=5,\n",
        "            num_channels=len(selected_indices) // 5,\n",
        "            adjacency_matrix=adjacency_matrix\n",
        "        ).to(device)\n",
        "        model.load_state_dict(torch.load(f'models/best_model_fold{fold+1}.pt'))\n",
        "        model.eval()\n",
        "\n",
        "        fold_preds = []\n",
        "        fold_probs = []\n",
        "        with torch.no_grad():\n",
        "            for inputs, _ in full_loader:\n",
        "                inputs = inputs.to(device)\n",
        "                outputs = model(inputs)\n",
        "                preds = (outputs > 0.5).float()\n",
        "                fold_preds.extend(preds.cpu().numpy())\n",
        "                fold_probs.extend(outputs.cpu().numpy())\n",
        "\n",
        "        ensemble_preds += np.array(fold_preds)\n",
        "        ensemble_probs += np.array(fold_probs)\n",
        "\n",
        "    simplified_model = SimplifiedEEGClassifier(\n",
        "        num_bands=5,\n",
        "        num_channels=len(selected_indices) // 5,\n",
        "        adjacency_matrix=adjacency_matrix\n",
        "    ).to(device)\n",
        "\n",
        "    train_dataset = EEGDataset(torch.FloatTensor(X_reshaped), torch.FloatTensor(y))\n",
        "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "    criterion = nn.BCELoss()\n",
        "    optimizer = optim.AdamW(simplified_model.parameters(), lr=0.0005, weight_decay=5e-4)\n",
        "\n",
        "    for epoch in range(50):\n",
        "        simplified_model.train()\n",
        "        for inputs, targets in train_loader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = simplified_model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "    torch.save(simplified_model.state_dict(), 'models/simplified_model.pt')\n",
        "\n",
        "    simplified_model.eval()\n",
        "    simple_preds = []\n",
        "    simple_probs = []\n",
        "    with torch.no_grad():\n",
        "        for inputs, _ in full_loader:\n",
        "            inputs = inputs.to(device)\n",
        "            outputs = simplified_model(inputs)\n",
        "            preds = (outputs > 0.5).float()\n",
        "            simple_preds.extend(preds.cpu().numpy())\n",
        "            simple_probs.extend(outputs.cpu().numpy())\n",
        "\n",
        "    ensemble_preds += np.array(simple_preds)\n",
        "    ensemble_probs += np.array(simple_probs)\n",
        "\n",
        "    ensemble_probs /= (n_splits + 1)\n",
        "    ensemble_preds_binary = (ensemble_probs > 0.5).astype(float)\n",
        "\n",
        "    accuracy = accuracy_score(y, ensemble_preds_binary)\n",
        "    f1 = f1_score(y, ensemble_preds_binary)\n",
        "    precision = precision_score(y, ensemble_preds_binary)\n",
        "    recall = recall_score(y, ensemble_preds_binary)\n",
        "    mcc = matthews_corrcoef(y, ensemble_preds_binary)\n",
        "    balanced_acc = balanced_accuracy_score(y, ensemble_preds_binary)\n",
        "    kappa = cohen_kappa_score(y, ensemble_preds_binary)\n",
        "    conf_matrix = confusion_matrix(y, ensemble_preds_binary)\n",
        "\n",
        "    auc_score = plot_roc_curve(y, ensemble_probs)\n",
        "\n",
        "    print(\"\\nEnsemble Model Results:\")\n",
        "    print(f\"Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"F1 Score: {f1:.4f}\")\n",
        "    print(f\"Precision: {precision:.4f}\")\n",
        "    print(f\"Recall: {recall:.4f}\")\n",
        "    print(f\"MCC: {mcc:.4f}\")\n",
        "    print(f\"Balanced Accuracy: {balanced_acc:.4f}\")\n",
        "    print(f\"Cohen's Kappa: {kappa:.4f}\")\n",
        "    print(f\"AUC: {auc_score:.4f}\")\n",
        "    print(f\"Confusion Matrix:\\n{conf_matrix}\")\n",
        "\n",
        "    metrics = {\n",
        "        'accuracy': accuracy,\n",
        "        'f1': f1,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'mcc': mcc,\n",
        "        'balanced_acc': balanced_acc,\n",
        "        'kappa': kappa,\n",
        "        'auc': auc_score\n",
        "    }\n",
        "\n",
        "    return metrics, conf_matrix, ensemble_probs\n",
        "\n",
        "def visualize_feature_activation(model, X, fold_num=None):\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        if isinstance(X, np.ndarray):\n",
        "            X_tensor = torch.FloatTensor(X).to(device)\n",
        "        else:\n",
        "            X_tensor = X.to(device)\n",
        "\n",
        "        outputs = model(X_tensor)\n",
        "\n",
        "        attention_weights = model.get_attention_weights()\n",
        "\n",
        "        if 'channel' in attention_weights:\n",
        "            channel_weights = attention_weights['channel'].mean(dim=0).cpu().numpy()\n",
        "\n",
        "            plt.figure(figsize=(12, 6))\n",
        "            plt.bar(range(len(channel_weights)), channel_weights)\n",
        "            plt.xlabel('Channel Index')\n",
        "            plt.ylabel('Attention Weight')\n",
        "            plt.title(f'Channel Attention Weights{\" - Fold \" + str(fold_num) if fold_num else \"\"}')\n",
        "            plt.tight_layout()\n",
        "            plt.savefig(f'visualization/channel_attention{\"_fold_\" + str(fold_num) if fold_num else \"\"}.png')\n",
        "            plt.close()\n",
        "\n",
        "            visualize_electrode_map(\n",
        "                channel_weights,\n",
        "                title=f'Electrode Importance Map{\" - Fold \" + str(fold_num) if fold_num else \"\"}'\n",
        "            )\n",
        "\n",
        "        if 'band' in attention_weights:\n",
        "            band_weights = attention_weights['band'].mean(dim=0).cpu().numpy()\n",
        "\n",
        "            plt.figure(figsize=(10, 6))\n",
        "            plt.bar(['Alpha', 'Beta', 'Delta', 'Theta', 'Gamma'], band_weights)\n",
        "            plt.xlabel('Frequency Band')\n",
        "            plt.ylabel('Attention Weight')\n",
        "            plt.title(f'Band Attention Weights{\" - Fold \" + str(fold_num) if fold_num else \"\"}')\n",
        "            plt.tight_layout()\n",
        "            plt.savefig(f'visualization/band_attention{\"_fold_\" + str(fold_num) if fold_num else \"\"}.png')\n",
        "            plt.close()\n",
        "\n",
        "def perform_learning_curve_analysis(X, y, selected_indices, adjacency_matrix, device, n_splits=5):\n",
        "    X_reshaped = reshape_data_to_bands_channels(X, selected_indices)\n",
        "\n",
        "    train_sizes = np.linspace(0.2, 1.0, 5)\n",
        "\n",
        "    train_sizes_abs = []\n",
        "    train_scores = []\n",
        "    val_scores = []\n",
        "\n",
        "    kf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=SEED)\n",
        "\n",
        "    for train_idx, val_idx in kf.split(X_reshaped, y):\n",
        "        X_train_full, X_val = X_reshaped[train_idx], X_reshaped[val_idx]\n",
        "        y_train_full, y_val = y[train_idx], y[val_idx]\n",
        "\n",
        "        fold_train_sizes = []\n",
        "        fold_train_scores = []\n",
        "        fold_val_scores = []\n",
        "\n",
        "        for train_size in train_sizes:\n",
        "            n_train_samples = int(train_size * len(X_train_full))\n",
        "            fold_train_sizes.append(n_train_samples)\n",
        "\n",
        "            indices = np.random.choice(len(X_train_full), n_train_samples, replace=False)\n",
        "            X_train_subset = X_train_full[indices]\n",
        "            y_train_subset = y_train_full[indices]\n",
        "\n",
        "            train_dataset = EEGDataset(torch.FloatTensor(X_train_subset), torch.FloatTensor(y_train_subset))\n",
        "            val_dataset = EEGDataset(torch.FloatTensor(X_val), torch.FloatTensor(y_val))\n",
        "\n",
        "            train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "            val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
        "\n",
        "            model = EEGClassifier(\n",
        "                num_bands=5,\n",
        "                num_channels=len(selected_indices) // 5,\n",
        "                adjacency_matrix=adjacency_matrix\n",
        "            ).to(device)\n",
        "\n",
        "            criterion = nn.BCELoss()\n",
        "            optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
        "\n",
        "            for epoch in range(30):\n",
        "                model.train()\n",
        "                for inputs, targets in train_loader:\n",
        "                    inputs, targets = inputs.to(device), targets.to(device)\n",
        "                    optimizer.zero_grad()\n",
        "                    outputs = model(inputs)\n",
        "                    loss = criterion(outputs, targets)\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "\n",
        "            model.eval()\n",
        "            train_metrics = calculate_metrics(model, train_loader, criterion, device)\n",
        "            val_metrics = calculate_metrics(model, val_loader, criterion, device)\n",
        "\n",
        "            fold_train_scores.append(train_metrics['accuracy'])\n",
        "            fold_val_scores.append(val_metrics['accuracy'])\n",
        "\n",
        "        train_sizes_abs.append(fold_train_sizes)\n",
        "        train_scores.append(fold_train_scores)\n",
        "        val_scores.append(fold_val_scores)\n",
        "\n",
        "    train_sizes_abs = np.mean(train_sizes_abs, axis=0)\n",
        "    train_scores = np.mean(train_scores, axis=0)\n",
        "    val_scores = np.mean(val_scores, axis=0)\n",
        "\n",
        "    plot_learning_curve(train_sizes_abs, train_scores, val_scores)\n",
        "\n",
        "    return train_sizes_abs, train_scores, val_scores\n",
        "\n",
        "def train_and_evaluate():\n",
        "    results_dir = f\"results_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
        "    os.makedirs(results_dir, exist_ok=True)\n",
        "\n",
        "    print(\"Loading data...\")\n",
        "    df = pd.read_csv('/eeg data.csv')\n",
        "\n",
        "    print(\"\\nDataset Information:\")\n",
        "    print(f\"Shape: {df.shape}\")\n",
        "    print(f\"Columns: {df.columns.tolist()}\")\n",
        "\n",
        "    data_summary = df.describe()\n",
        "    data_summary.to_csv(f'{results_dir}/data_summary.csv')\n",
        "\n",
        "    X = df.iloc[:, 1:-1].values\n",
        "    y = df.iloc[:, -1].values\n",
        "    y = y.astype(np.float32)\n",
        "\n",
        "    feature_names = df.columns[1:-1].tolist()\n",
        "\n",
        "    data_stats, class_distribution = visualize_dataset(X, y, feature_names)\n",
        "    data_stats.to_csv(f'{results_dir}/feature_stats.csv')\n",
        "    class_distribution.to_csv(f'{results_dir}/class_distribution.csv')\n",
        "\n",
        "    print(\"\\nClass Distribution:\")\n",
        "    print(class_distribution)\n",
        "\n",
        "    print(\"\\nPerforming feature selection...\")\n",
        "    selector = SelectKBest(f_classif, k=SELECTED_FEATURES)\n",
        "    X_selected = selector.fit_transform(X, y)\n",
        "    selected_indices = selector.get_support(indices=True)\n",
        "\n",
        "    feature_scores = selector.scores_\n",
        "    selected_features_df = visualize_feature_importance(X, selected_indices, feature_scores)\n",
        "    selected_features_df.to_csv(f'{results_dir}/selected_features.csv')\n",
        "\n",
        "    print(f\"Top 10 selected features:\")\n",
        "    print(selected_features_df.head(10))\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    X_scaled = scaler.fit_transform(X_selected)\n",
        "\n",
        "    channels_per_band = len(selected_indices) // 5\n",
        "\n",
        "    adjacency_matrix = create_adjacency_matrix(CHANNEL_COORDS, channels_per_band)\n",
        "\n",
        "    X_aug, y_aug = augment_data(X_scaled, y)\n",
        "\n",
        "    n_splits = 5\n",
        "    kf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=SEED)\n",
        "    fold_results = []\n",
        "\n",
        "    fold_monitors = []\n",
        "    fold_confusion_matrices = []\n",
        "    all_fold_metrics = []\n",
        "    all_val_probs = []\n",
        "    all_val_targets = []\n",
        "\n",
        "    for fold, (train_idx, val_idx) in enumerate(kf.split(X_aug, y_aug)):\n",
        "        print(f\"\\nTraining fold {fold+1}/{n_splits}\")\n",
        "\n",
        "        monitor = MetricsTracker(fold+1, EPOCHS)\n",
        "        fold_monitors.append(monitor)\n",
        "\n",
        "        X_train, X_val = X_aug[train_idx], X_aug[val_idx]\n",
        "        y_train, y_val = y_aug[train_idx], y_aug[val_idx]\n",
        "\n",
        "        X_train_reshaped = reshape_data_to_bands_channels(X_train, selected_indices)\n",
        "        X_val_reshaped = reshape_data_to_bands_channels(X_val, selected_indices)\n",
        "\n",
        "        train_dataset = EEGDataset(torch.FloatTensor(X_train_reshaped), torch.FloatTensor(y_train))\n",
        "        val_dataset = EEGDataset(torch.FloatTensor(X_val_reshaped), torch.FloatTensor(y_val))\n",
        "\n",
        "        class_counts = np.bincount(y_train.astype(int))\n",
        "        class_weights = 1. / torch.tensor(class_counts, dtype=torch.float)\n",
        "        sample_weights = class_weights[y_train.astype(int)]\n",
        "        sampler = WeightedRandomSampler(weights=sample_weights,\n",
        "                                        num_samples=len(sample_weights),\n",
        "                                        replacement=True)\n",
        "\n",
        "        train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, sampler=sampler)\n",
        "        val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
        "\n",
        "        model = EEGClassifier(\n",
        "            num_bands=5,\n",
        "            num_channels=len(selected_indices) // 5,\n",
        "            adjacency_matrix=adjacency_matrix\n",
        "        ).to(device)\n",
        "\n",
        "        criterion = nn.BCELoss()\n",
        "        optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
        "\n",
        "        scheduler = optim.lr_scheduler.CosineAnnealingLR(\n",
        "            optimizer,\n",
        "            T_max=30,\n",
        "            eta_min=1e-6\n",
        "        )\n",
        "\n",
        "        best_val_loss = float('inf')\n",
        "        early_stop_counter = 0\n",
        "        early_stop_patience = 10\n",
        "        min_epochs = 30\n",
        "        loss_improvement_threshold = 0.001\n",
        "\n",
        "        for epoch in range(EPOCHS):\n",
        "            model.train()\n",
        "            train_loss = 0.0\n",
        "\n",
        "            for inputs, targets in train_loader:\n",
        "                inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, targets)\n",
        "\n",
        "                loss.backward()\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "                optimizer.step()\n",
        "\n",
        "                train_loss += loss.item()\n",
        "\n",
        "            scheduler.step()\n",
        "\n",
        "            if (epoch + 1) % 10 == 0 or epoch == EPOCHS - 1:\n",
        "                train_metrics = calculate_metrics(model, train_loader, criterion, device)\n",
        "                val_metrics = calculate_metrics(model, val_loader, criterion, device)\n",
        "\n",
        "                monitor.update(epoch+1, train_metrics, val_metrics)\n",
        "\n",
        "                overfitting_ratio = val_metrics['loss'] / train_metrics['loss'] if train_metrics['loss'] > 0 else 1.0\n",
        "\n",
        "                print(f\"Epoch {epoch+1}/{EPOCHS}, Train Loss: {train_metrics['loss']:.4f}, \"\n",
        "                      f\"Val Loss: {val_metrics['loss']:.4f}, \"\n",
        "                      f\"Train Acc: {train_metrics['accuracy']:.4f}, Val Acc: {val_metrics['accuracy']:.4f}, \"\n",
        "                      f\"Train F1: {train_metrics['f1']:.4f}, Val F1: {val_metrics['f1']:.4f}, \"\n",
        "                      f\"Overfitting ratio: {overfitting_ratio:.4f}\")\n",
        "\n",
        "            val_loss = calculate_metrics(model, val_loader, criterion, device)['loss']\n",
        "\n",
        "            if epoch >= min_epochs:\n",
        "                if val_loss < best_val_loss - loss_improvement_threshold:\n",
        "                    best_val_loss = val_loss\n",
        "                    early_stop_counter = 0\n",
        "                    torch.save(model.state_dict(), f'models/best_model_fold{fold+1}.pt')\n",
        "                else:\n",
        "                    early_stop_counter += 1\n",
        "                    if early_stop_counter >= early_stop_patience:\n",
        "                        print(f\"Early stopping at epoch {epoch+1}\")\n",
        "                        break\n",
        "            elif val_loss < best_val_loss:\n",
        "                best_val_loss = val_loss\n",
        "                torch.save(model.state_dict(), f'models/best_model_fold{fold+1}.pt')\n",
        "\n",
        "        model.load_state_dict(torch.load(f'models/best_model_fold{fold+1}.pt'))\n",
        "        model.eval()\n",
        "\n",
        "        final_val_metrics = calculate_metrics(model, val_loader, criterion, device)\n",
        "        all_fold_metrics.append(final_val_metrics)\n",
        "        all_val_probs.extend(final_val_metrics['probabilities'])\n",
        "        all_val_targets.extend(final_val_metrics['targets'])\n",
        "\n",
        "        final_train_metrics = calculate_metrics(model, train_loader, criterion, device)\n",
        "\n",
        "        conf_matrix = confusion_matrix(final_val_metrics['targets'], final_val_metrics['predictions'])\n",
        "        fold_confusion_matrices.append(conf_matrix)\n",
        "\n",
        "        auc_score = plot_roc_curve(final_val_metrics['targets'], final_val_metrics['probabilities'], fold+1)\n",
        "\n",
        "        visualize_feature_activation(model, X_val_reshaped, fold+1)\n",
        "\n",
        "        print(f\"\\nFold {fold+1} Results:\")\n",
        "        print(f\"Accuracy: {final_val_metrics['accuracy']:.4f}\")\n",
        "        print(f\"F1 Score: {final_val_metrics['f1']:.4f}\")\n",
        "        print(f\"Precision: {final_val_metrics['precision']:.4f}\")\n",
        "        print(f\"Recall: {final_val_metrics['recall']:.4f}\")\n",
        "        print(f\"MCC: {final_val_metrics['mcc']:.4f}\")\n",
        "        print(f\"Balanced Accuracy: {final_val_metrics['balanced_acc']:.4f}\")\n",
        "        print(f\"Cohen's Kappa: {final_val_metrics['kappa']:.4f}\")\n",
        "        print(f\"AUC: {auc_score:.4f}\")\n",
        "        print(f\"Confusion Matrix:\\n{conf_matrix}\")\n",
        "\n",
        "        overfitting_ratio = final_val_metrics['loss'] / final_train_metrics['loss']\n",
        "        print(f\"Overfitting ratio (val_loss/train_loss): {overfitting_ratio:.4f}\")\n",
        "        print(f\"Target ratio should be close to 1.0. Ratio > 1.5 indicates overfitting\")\n",
        "\n",
        "        monitor.plot_metrics()\n",
        "        overfitting_status = monitor.analyze_overfitting()\n",
        "        print(f\"Overfitting analysis for fold {fold+1}: {overfitting_status}\")\n",
        "\n",
        "        fold_results.append({\n",
        "            'fold': fold + 1,\n",
        "            'accuracy': final_val_metrics['accuracy'],\n",
        "            'f1': final_val_metrics['f1'],\n",
        "            'precision': final_val_metrics['precision'],\n",
        "            'recall': final_val_metrics['recall'],\n",
        "            'mcc': final_val_metrics['mcc'],\n",
        "            'balanced_accuracy': final_val_metrics['balanced_acc'],\n",
        "            'kappa': final_val_metrics['kappa'],\n",
        "            'auc': auc_score,\n",
        "            'overfitting_ratio': overfitting_ratio,\n",
        "            'overfitting_status': overfitting_status\n",
        "        })\n",
        "\n",
        "    overall_auc = plot_roc_curve(np.array(all_val_targets), np.array(all_val_probs))\n",
        "\n",
        "    fold_results_df = pd.DataFrame(fold_results)\n",
        "    fold_results_df.to_csv(f'{results_dir}/fold_results.csv', index=False)\n",
        "    numeric_cols = fold_results_df.select_dtypes(include=['number']).columns\n",
        "\n",
        "    print(\"\\nOverall Results:\")\n",
        "    print(f\"Mean Accuracy: {fold_results_df['accuracy'].mean():.4f} ± {fold_results_df['accuracy'].std():.4f}\")\n",
        "    print(f\"Mean F1 Score: {fold_results_df['f1'].mean():.4f} ± {fold_results_df['f1'].std():.4f}\")\n",
        "    print(f\"Mean AUC: {fold_results_df['auc'].mean():.4f} ± {fold_results_df['auc'].std():.4f}\")\n",
        "\n",
        "    status_counts = fold_results_df['overfitting_status'].value_counts()\n",
        "    print(\"\\nOverfitting Status Counts:\")\n",
        "    for status, count in status_counts.items():\n",
        "        print(f\"{status}: {count}\")\n",
        "\n",
        "    plot_cross_val_metrics(fold_monitors)\n",
        "\n",
        "    print(\"\\nPerforming learning curve analysis...\")\n",
        "    train_sizes, train_scores, val_scores = perform_learning_curve_analysis(\n",
        "        X_scaled, y, selected_indices, adjacency_matrix, device)\n",
        "\n",
        "    ensemble_metrics, ensemble_conf_matrix, ensemble_probs = create_pytorch_ensemble(\n",
        "        X_scaled, y, selected_indices, adjacency_matrix, device, n_splits\n",
        "    )\n",
        "\n",
        "    plot_confusion_matrices(fold_confusion_matrices, ensemble_conf_matrix)\n",
        "\n",
        "    print(\"\\nOverfitting Analysis Summary:\")\n",
        "    for i, monitor in enumerate(fold_monitors):\n",
        "        status = monitor.analyze_overfitting()\n",
        "        print(f\"Fold {i+1}: {status}\")\n",
        "\n",
        "    avg_final_train_acc = np.mean([m.train_accs[-1] for m in fold_monitors])\n",
        "    avg_final_val_acc = np.mean([m.val_accs[-1] for m in fold_monitors])\n",
        "    generalization_gap = avg_final_train_acc - avg_final_val_acc\n",
        "\n",
        "    print(f\"\\nFinal Generalization Gap (Train Acc - Val Acc): {generalization_gap:.4f}\")\n",
        "    if generalization_gap > 0.05:\n",
        "        print(\"Warning: Model shows signs of overfitting (Train accuracy significantly higher than validation)\")\n",
        "    elif avg_final_val_acc > 0.99 and avg_final_train_acc > 0.99:\n",
        "        print(\"Warning: Perfect accuracy on both training and validation may indicate:\")\n",
        "        print(\"1. Model is memorizing the dataset (especially if the dataset is small)\")\n",
        "        print(\"2. Possible data leakage between training and validation sets\")\n",
        "        print(\"3. The task may be too easy or the dataset too simple\")\n",
        "        print(\"Consider testing on a completely separate dataset to verify generalization\")\n",
        "    else:\n",
        "        print(\"No significant signs of overfitting detected based on the accuracy gap\")\n",
        "\n",
        "    avg_final_train_loss = np.mean([m.train_losses[-1] for m in fold_monitors])\n",
        "    avg_final_val_loss = np.mean([m.val_losses[-1] for m in fold_monitors])\n",
        "    loss_gap = avg_final_val_loss - avg_final_train_loss\n",
        "    avg_overfitting_ratio = avg_final_val_loss / avg_final_train_loss if avg_final_train_loss > 0 else 1.0\n",
        "\n",
        "    print(f\"Final Loss Gap (Val Loss - Train Loss): {loss_gap:.4f}\")\n",
        "    print(f\"Average Overfitting Ratio (Val Loss / Train Loss): {avg_overfitting_ratio:.4f}\")\n",
        "\n",
        "    if avg_overfitting_ratio > 1.5:\n",
        "        print(\"Warning: Average overfitting ratio above 1.5 indicates overfitting\")\n",
        "    elif avg_overfitting_ratio > 1.2:\n",
        "        print(\"Mild overfitting detected (ratio between 1.2 and 1.5)\")\n",
        "    else:\n",
        "        print(\"Good generalization (ratio close to 1.0)\")\n",
        "\n",
        "    results = {\n",
        "        'cross_val': {\n",
        "            'accuracy': fold_results_df['accuracy'].mean(),\n",
        "            'accuracy_std': fold_results_df['accuracy'].std(),\n",
        "            'f1': fold_results_df['f1'].mean(),\n",
        "            'f1_std': fold_results_df['f1'].std(),\n",
        "            'precision': fold_results_df['precision'].mean(),\n",
        "            'precision_std': fold_results_df['precision'].std(),\n",
        "            'recall': fold_results_df['recall'].mean(),\n",
        "            'recall_std': fold_results_df['recall'].std(),\n",
        "            'mcc': fold_results_df['mcc'].mean(),\n",
        "            'mcc_std': fold_results_df['mcc'].std(),\n",
        "            'balanced_accuracy': fold_results_df['balanced_accuracy'].mean(),\n",
        "            'balanced_accuracy_std': fold_results_df['balanced_accuracy'].std(),\n",
        "            'kappa': fold_results_df['kappa'].mean(),\n",
        "            'kappa_std': fold_results_df['kappa'].std(),\n",
        "            'auc': fold_results_df['auc'].mean(),\n",
        "            'auc_std': fold_results_df['auc'].std(),\n",
        "            'overfitting_ratio': fold_results_df['overfitting_ratio'].mean(),\n",
        "            'overfitting_ratio_std': fold_results_df['overfitting_ratio'].std()\n",
        "        },\n",
        "        'ensemble': ensemble_metrics\n",
        "    }\n",
        "\n",
        "    results_df = pd.DataFrame({\n",
        "        'Metric': ['Accuracy', 'F1 Score', 'Precision', 'Recall', 'MCC', 'Balanced Accuracy', 'Cohen\\'s Kappa', 'AUC'],\n",
        "        'Cross-Validation (Mean)': [\n",
        "            results['cross_val']['accuracy'],\n",
        "            results['cross_val']['f1'],\n",
        "            results['cross_val']['precision'],\n",
        "            results['cross_val']['recall'],\n",
        "            results['cross_val']['mcc'],\n",
        "            results['cross_val']['balanced_accuracy'],\n",
        "            results['cross_val']['kappa'],\n",
        "            results['cross_val']['auc']\n",
        "        ],\n",
        "        'Cross-Validation (Std)': [\n",
        "            results['cross_val']['accuracy_std'],\n",
        "            results['cross_val']['f1_std'],\n",
        "            results['cross_val']['precision_std'],\n",
        "            results['cross_val']['recall_std'],\n",
        "            results['cross_val']['mcc_std'],\n",
        "            results['cross_val']['balanced_accuracy_std'],\n",
        "            results['cross_val']['kappa_std'],\n",
        "            results['cross_val']['auc_std']\n",
        "        ],\n",
        "        'Ensemble': [\n",
        "            results['ensemble']['accuracy'],\n",
        "            results['ensemble']['f1'],\n",
        "            results['ensemble']['precision'],\n",
        "            results['ensemble']['recall'],\n",
        "            results['ensemble']['mcc'],\n",
        "            results['ensemble']['balanced_acc'],\n",
        "            results['ensemble']['kappa'],\n",
        "            results['ensemble']['auc']\n",
        "        ]\n",
        "    })\n",
        "\n",
        "    results_df.to_csv(f'{results_dir}/final_results.csv', index=False)\n",
        "\n",
        "    print(\"\\nFinal Results Summary:\")\n",
        "    print(results_df)\n",
        "\n",
        "    return results\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    start_time = time.time()\n",
        "    results = train_and_evaluate()\n",
        "    end_time = time.time()\n",
        "\n",
        "    print(f\"\\nTotal execution time: {(end_time - start_time) / 60:.2f} minutes\")"
      ]
    }
  ]
}