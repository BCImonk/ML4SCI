{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sRo1xQjfq6cN",
        "outputId": "53f0bd5d-dd44-45bc-c55d-464d3da3efa4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n",
            "Loading data...\n",
            "Performing feature selection...\n",
            "\n",
            "Training fold 1/5\n",
            "Epoch 10/100, Train Loss: 0.6991, Val Loss: 0.7322, Accuracy: 0.3750, F1: 0.0000\n",
            "Epoch 20/100, Train Loss: 0.6761, Val Loss: 0.7308, Accuracy: 0.3750, F1: 0.0000\n",
            "Epoch 30/100, Train Loss: 0.7162, Val Loss: 0.7280, Accuracy: 0.3750, F1: 0.0000\n",
            "Epoch 40/100, Train Loss: 0.7908, Val Loss: 0.7285, Accuracy: 0.3750, F1: 0.0000\n",
            "Epoch 50/100, Train Loss: 0.7082, Val Loss: 0.7274, Accuracy: 0.3750, F1: 0.0000\n",
            "Epoch 60/100, Train Loss: 0.7047, Val Loss: 0.7273, Accuracy: 0.3750, F1: 0.0000\n",
            "Epoch 70/100, Train Loss: 0.7275, Val Loss: 0.7272, Accuracy: 0.3750, F1: 0.0000\n",
            "Epoch 80/100, Train Loss: 0.7037, Val Loss: 0.7273, Accuracy: 0.3750, F1: 0.0000\n",
            "Epoch 90/100, Train Loss: 0.6925, Val Loss: 0.7274, Accuracy: 0.3750, F1: 0.0000\n",
            "Early stopping at epoch 95\n",
            "\n",
            "Fold 1 Results:\n",
            "Accuracy: 0.3750\n",
            "F1 Score: 0.0000\n",
            "Confusion Matrix:\n",
            "[[3 1]\n",
            " [4 0]]\n",
            "\n",
            "Training fold 2/5\n",
            "Epoch 10/100, Train Loss: 0.7697, Val Loss: 0.7279, Accuracy: 0.5000, F1: 0.5000\n",
            "Epoch 20/100, Train Loss: 0.7557, Val Loss: 0.7250, Accuracy: 0.5000, F1: 0.3333\n",
            "Epoch 30/100, Train Loss: 0.7181, Val Loss: 0.7211, Accuracy: 0.5000, F1: 0.0000\n",
            "Epoch 40/100, Train Loss: 0.7442, Val Loss: 0.7193, Accuracy: 0.5000, F1: 0.0000\n",
            "Epoch 50/100, Train Loss: 0.7799, Val Loss: 0.7180, Accuracy: 0.5000, F1: 0.0000\n",
            "Epoch 60/100, Train Loss: 0.7129, Val Loss: 0.7166, Accuracy: 0.5000, F1: 0.0000\n",
            "Epoch 70/100, Train Loss: 0.7058, Val Loss: 0.7154, Accuracy: 0.5000, F1: 0.0000\n",
            "Epoch 80/100, Train Loss: 0.7305, Val Loss: 0.7153, Accuracy: 0.5000, F1: 0.0000\n",
            "Epoch 90/100, Train Loss: 0.7081, Val Loss: 0.7151, Accuracy: 0.5000, F1: 0.0000\n",
            "Epoch 100/100, Train Loss: 0.7047, Val Loss: 0.7151, Accuracy: 0.5000, F1: 0.0000\n",
            "\n",
            "Fold 2 Results:\n",
            "Accuracy: 0.5000\n",
            "F1 Score: 0.0000\n",
            "Confusion Matrix:\n",
            "[[4 0]\n",
            " [4 0]]\n",
            "\n",
            "Training fold 3/5\n",
            "Epoch 10/100, Train Loss: 0.6883, Val Loss: 0.7065, Accuracy: 0.5000, F1: 0.0000\n",
            "Epoch 20/100, Train Loss: 0.7140, Val Loss: 0.6981, Accuracy: 0.5000, F1: 0.0000\n",
            "Epoch 30/100, Train Loss: 0.7318, Val Loss: 0.6946, Accuracy: 0.5000, F1: 0.0000\n",
            "Epoch 40/100, Train Loss: 0.6703, Val Loss: 0.6908, Accuracy: 0.5000, F1: 0.0000\n",
            "Epoch 50/100, Train Loss: 0.7084, Val Loss: 0.6881, Accuracy: 0.5000, F1: 0.0000\n",
            "Epoch 60/100, Train Loss: 0.7177, Val Loss: 0.6874, Accuracy: 0.5000, F1: 0.0000\n",
            "Epoch 70/100, Train Loss: 0.6802, Val Loss: 0.6855, Accuracy: 0.5000, F1: 0.0000\n",
            "Epoch 80/100, Train Loss: 0.7215, Val Loss: 0.6844, Accuracy: 0.5000, F1: 0.0000\n",
            "Epoch 90/100, Train Loss: 0.7475, Val Loss: 0.6843, Accuracy: 0.5000, F1: 0.0000\n",
            "Epoch 100/100, Train Loss: 0.6852, Val Loss: 0.6843, Accuracy: 0.5000, F1: 0.0000\n",
            "\n",
            "Fold 3 Results:\n",
            "Accuracy: 0.5000\n",
            "F1 Score: 0.0000\n",
            "Confusion Matrix:\n",
            "[[4 0]\n",
            " [4 0]]\n",
            "\n",
            "Training fold 4/5\n",
            "Epoch 10/100, Train Loss: 0.7027, Val Loss: 0.7184, Accuracy: 0.5000, F1: 0.0000\n",
            "Epoch 20/100, Train Loss: 0.7537, Val Loss: 0.7126, Accuracy: 0.5000, F1: 0.0000\n",
            "Epoch 30/100, Train Loss: 0.6948, Val Loss: 0.7140, Accuracy: 0.5000, F1: 0.0000\n",
            "Epoch 40/100, Train Loss: 0.7049, Val Loss: 0.7124, Accuracy: 0.5000, F1: 0.0000\n",
            "Epoch 50/100, Train Loss: 0.6934, Val Loss: 0.7094, Accuracy: 0.6250, F1: 0.4000\n",
            "Epoch 60/100, Train Loss: 0.7621, Val Loss: 0.7068, Accuracy: 0.6250, F1: 0.4000\n",
            "Epoch 70/100, Train Loss: 0.6976, Val Loss: 0.7062, Accuracy: 0.6250, F1: 0.4000\n",
            "Epoch 80/100, Train Loss: 0.7453, Val Loss: 0.7054, Accuracy: 0.6250, F1: 0.4000\n",
            "Epoch 90/100, Train Loss: 0.7798, Val Loss: 0.7059, Accuracy: 0.6250, F1: 0.4000\n",
            "Epoch 100/100, Train Loss: 0.7235, Val Loss: 0.7059, Accuracy: 0.6250, F1: 0.4000\n",
            "\n",
            "Fold 4 Results:\n",
            "Accuracy: 0.6250\n",
            "F1 Score: 0.4000\n",
            "Confusion Matrix:\n",
            "[[4 0]\n",
            " [3 1]]\n",
            "\n",
            "Training fold 5/5\n",
            "Epoch 10/100, Train Loss: 0.6641, Val Loss: 0.7239, Accuracy: 0.3750, F1: 0.0000\n",
            "Epoch 20/100, Train Loss: 0.7054, Val Loss: 0.7220, Accuracy: 0.3750, F1: 0.0000\n",
            "Epoch 30/100, Train Loss: 0.7032, Val Loss: 0.7195, Accuracy: 0.5000, F1: 0.0000\n",
            "Epoch 40/100, Train Loss: 0.7725, Val Loss: 0.7159, Accuracy: 0.5000, F1: 0.0000\n",
            "Epoch 50/100, Train Loss: 0.7079, Val Loss: 0.7131, Accuracy: 0.5000, F1: 0.0000\n",
            "Epoch 60/100, Train Loss: 0.7079, Val Loss: 0.7120, Accuracy: 0.5000, F1: 0.0000\n",
            "Epoch 70/100, Train Loss: 0.6907, Val Loss: 0.7116, Accuracy: 0.5000, F1: 0.0000\n",
            "Epoch 80/100, Train Loss: 0.6762, Val Loss: 0.7114, Accuracy: 0.5000, F1: 0.0000\n",
            "Epoch 90/100, Train Loss: 0.7521, Val Loss: 0.7113, Accuracy: 0.5000, F1: 0.0000\n",
            "Epoch 100/100, Train Loss: 0.6554, Val Loss: 0.7112, Accuracy: 0.5000, F1: 0.0000\n",
            "\n",
            "Fold 5 Results:\n",
            "Accuracy: 0.5000\n",
            "F1 Score: 0.0000\n",
            "Confusion Matrix:\n",
            "[[4 0]\n",
            " [4 0]]\n",
            "\n",
            "Overall Results:\n",
            "Mean CV Accuracy: 0.5000 ± 0.0791\n",
            "Mean CV F1 Score: 0.0800 ± 0.1600\n",
            "OOF Accuracy: 0.5000\n",
            "OOF F1 Score: 0.0909\n",
            "\n",
            "Creating ensemble of models...\n",
            "\n",
            "Ensemble Model Results:\n",
            "Accuracy: 0.5250\n",
            "F1 Score: 0.0952\n",
            "Confusion Matrix:\n",
            "[[20  0]\n",
            " [19  1]]\n",
            "\n",
            "Total execution time: 0.18 minutes\n",
            "Final Accuracy: 0.5250\n",
            "Final F1 Score: 0.0952\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
        "from sklearn.feature_selection import SelectKBest, f_classif\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "\n",
        "SEED = 42\n",
        "torch.manual_seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(SEED)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "BATCH_SIZE = 8\n",
        "EPOCHS = 100\n",
        "LEARNING_RATE = 5e-4\n",
        "NUM_CHANNELS = 64\n",
        "NUM_BANDS = 5\n",
        "CHANNEL_NAMES = [f\"{band}{i}\" for band in [\"alpha\", \"beta\", \"delta\", \"theta\", \"gamma\"] for i in range(1, 65)]\n",
        "\n",
        "CHANNEL_COORDS = {\n",
        "    1: (0.75, 0.15), 2: (0.65, 0.2), 3: (0.55, 0.25), 4: (0.45, 0.3), 5: (0.35, 0.2),\n",
        "    6: (0.25, 0.2), 7: (0.4, 0.35), 8: (0.5, 0.15), 9: (0.45, 0.25), 10: (0.35, 0.15),\n",
        "    11: (0.3, 0.25), 12: (0.4, 0.3), 13: (0.25, 0.25), 14: (0.35, 0.3), 15: (0.3, 0.35),\n",
        "    16: (0.35, 0.4), 17: (0.2, 0.2), 18: (0.25, 0.3), 19: (0.2, 0.3), 20: (0.3, 0.4),\n",
        "    21: (0.4, 0.45), 22: (0.25, 0.4), 23: (0.15, 0.35), 24: (0.2, 0.4), 25: (0.25, 0.45),\n",
        "    26: (0.3, 0.5), 27: (0.2, 0.5), 28: (0.25, 0.55), 29: (0.15, 0.55), 30: (0.2, 0.6),\n",
        "    31: (0.35, 0.55), 32: (0.15, 0.65), 33: (0.3, 0.65), 34: (0.45, 0.35), 35: (0.4, 0.7),\n",
        "    36: (0.35, 0.6), 37: (0.4, 0.65), 38: (0.35, 0.65), 39: (0.45, 0.7), 40: (0.45, 0.6),\n",
        "    41: (0.55, 0.45), 42: (0.55, 0.6), 43: (0.6, 0.65), 44: (0.65, 0.65), 45: (0.6, 0.55),\n",
        "    46: (0.55, 0.5), 47: (0.65, 0.6), 48: (0.55, 0.55), 49: (0.6, 0.4), 50: (0.6, 0.35),\n",
        "    51: (0.55, 0.4), 52: (0.65, 0.45), 53: (0.65, 0.35), 54: (0.55, 0.35), 55: (0.7, 0.4),\n",
        "    56: (0.65, 0.3), 57: (0.6, 0.3), 58: (0.7, 0.25), 59: (0.65, 0.25), 60: (0.55, 0.3),\n",
        "    61: (0.75, 0.2), 62: (0.5, 0.1), 63: (0.45, 0.05), 64: (0.15, 0.2)\n",
        "}\n",
        "\n",
        "class EEGDataset(Dataset):\n",
        "    def __init__(self, features, targets):\n",
        "        self.features = features\n",
        "        self.targets = targets\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.targets)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.features[idx], self.targets[idx]\n",
        "\n",
        "class SimpleEEGModel(nn.Module):\n",
        "    def __init__(self, input_features):\n",
        "        super(SimpleEEGModel, self).__init__()\n",
        "\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(input_features, 64),\n",
        "            nn.LeakyReLU(0.1),\n",
        "            nn.Dropout(0.5),\n",
        "\n",
        "            nn.Linear(64, 32),\n",
        "            nn.LeakyReLU(0.1),\n",
        "            nn.Dropout(0.5),\n",
        "\n",
        "            nn.Linear(32, 16),\n",
        "            nn.LeakyReLU(0.1),\n",
        "            nn.Dropout(0.4),\n",
        "\n",
        "            nn.Linear(16, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.xavier_normal_(m.weight)\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0.0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x).squeeze(1)\n",
        "\n",
        "def augment_data(X, y, noise_levels=[0.01, 0.02, 0.05], num_per_level=1):\n",
        "    X_aug, y_aug = X.copy(), y.copy()\n",
        "\n",
        "    for noise_level in noise_levels:\n",
        "        for _ in range(num_per_level):\n",
        "            noise = np.random.normal(0, noise_level, X.shape)\n",
        "            X_noisy = X + noise\n",
        "\n",
        "            X_aug = np.vstack((X_aug, X_noisy))\n",
        "            y_aug = np.append(y_aug, y)\n",
        "\n",
        "            # Add horizontal flips for selected features\n",
        "            if X.shape[1] > 10:  # Only if we have enough features\n",
        "                flip_indices = np.random.choice(X.shape[1], size=int(X.shape[1]*0.1), replace=False)\n",
        "                X_flipped = X.copy()\n",
        "                X_flipped[:, flip_indices] = -X_flipped[:, flip_indices]\n",
        "                X_aug = np.vstack((X_aug, X_flipped))\n",
        "                y_aug = np.append(y_aug, y)\n",
        "\n",
        "    # Mix samples within same class (mixup-like augmentation)\n",
        "    for c in np.unique(y):\n",
        "        class_indices = np.where(y == c)[0]\n",
        "        if len(class_indices) >= 2:\n",
        "            for _ in range(min(10, len(class_indices))):\n",
        "                idx1, idx2 = np.random.choice(class_indices, 2, replace=False)\n",
        "                alpha = np.random.beta(0.4, 0.4)\n",
        "                mixed_sample = alpha * X[idx1] + (1 - alpha) * X[idx2]\n",
        "                X_aug = np.vstack((X_aug, mixed_sample.reshape(1, -1)))\n",
        "                y_aug = np.append(y_aug, c)\n",
        "\n",
        "    return X_aug, y_aug\n",
        "\n",
        "def train_and_evaluate():\n",
        "    print(\"Loading data...\")\n",
        "    df = pd.read_csv('/eeg data.csv')\n",
        "\n",
        "    X = df.iloc[:, 1:-1].values\n",
        "    y = df.iloc[:, -1].values\n",
        "    y = y.astype(np.float32)\n",
        "\n",
        "    print(\"Performing feature selection...\")\n",
        "    selector = SelectKBest(f_classif, k=40)  # Reduced from 200 to 40 features\n",
        "    X_selected = selector.fit_transform(X, y)\n",
        "    selected_indices = selector.get_support(indices=True)\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    X_scaled = scaler.fit_transform(X_selected)\n",
        "\n",
        "    X_aug, y_aug = augment_data(X_scaled, y, noise_levels=[0.01, 0.03, 0.05, 0.08, 0.1], num_per_level=2)\n",
        "\n",
        "    n_splits = 5\n",
        "    kf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=SEED)\n",
        "    fold_results = []\n",
        "\n",
        "    val_fold_predictions = np.zeros((len(X_scaled), n_splits))\n",
        "    oof_indices = []\n",
        "\n",
        "    for fold, (train_idx, val_idx) in enumerate(kf.split(X_scaled, y)):\n",
        "        print(f\"\\nTraining fold {fold+1}/{n_splits}\")\n",
        "\n",
        "        # Split data\n",
        "        X_train, X_val = X_aug[train_idx], X_scaled[val_idx]\n",
        "        y_train, y_val = y_aug[train_idx], y[val_idx]\n",
        "\n",
        "        # Store indices for OOF predictions\n",
        "        oof_indices.append(val_idx)\n",
        "\n",
        "        train_dataset = EEGDataset(torch.FloatTensor(X_train), torch.FloatTensor(y_train))\n",
        "        val_dataset = EEGDataset(torch.FloatTensor(X_val), torch.FloatTensor(y_val))\n",
        "\n",
        "        # Handle class imbalance with weighted sampling\n",
        "        class_counts = np.bincount(y_train.astype(int))\n",
        "        class_weights = 1. / torch.tensor(class_counts, dtype=torch.float)\n",
        "        sample_weights = class_weights[y_train.astype(int)]\n",
        "        sampler = WeightedRandomSampler(weights=sample_weights,\n",
        "                                        num_samples=len(sample_weights),\n",
        "                                        replacement=True)\n",
        "\n",
        "        train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, sampler=sampler)\n",
        "        val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
        "\n",
        "        # Initialize simpler model\n",
        "        model = SimpleEEGModel(input_features=X_train.shape[1]).to(device)\n",
        "\n",
        "        # Define loss and optimizer\n",
        "        criterion = nn.BCEWithLogitsLoss()  # More numerically stable than BCE\n",
        "        optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-2)\n",
        "        scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS)\n",
        "\n",
        "        # Training loop\n",
        "        best_val_loss = float('inf')\n",
        "        early_stop_counter = 0\n",
        "        early_stop_patience = 30\n",
        "        val_predictions = np.zeros(len(X_val))\n",
        "\n",
        "        for epoch in range(EPOCHS):\n",
        "            # Training phase\n",
        "            model.train()\n",
        "            train_loss = 0.0\n",
        "\n",
        "            for inputs, targets in train_loader:\n",
        "                inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "                # Forward pass\n",
        "                optimizer.zero_grad()\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, targets)\n",
        "\n",
        "                # L1 regularization\n",
        "                l1_lambda = 1e-5\n",
        "                l1_norm = sum(p.abs().sum() for p in model.parameters())\n",
        "                loss = loss + l1_lambda * l1_norm\n",
        "\n",
        "                # Backward pass and optimize\n",
        "                loss.backward()\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "                optimizer.step()\n",
        "\n",
        "                train_loss += loss.item()\n",
        "\n",
        "            # Learning rate scheduler step\n",
        "            scheduler.step()\n",
        "\n",
        "            # Validation phase\n",
        "            model.eval()\n",
        "            val_loss = 0.0\n",
        "            all_preds = []\n",
        "            all_targets = []\n",
        "            fold_val_preds = []\n",
        "\n",
        "            with torch.no_grad():\n",
        "                for inputs, targets in val_loader:\n",
        "                    inputs, targets = inputs.to(device), targets.to(device)\n",
        "                    outputs = model(inputs)\n",
        "                    loss = criterion(outputs, targets)\n",
        "                    val_loss += loss.item()\n",
        "\n",
        "                    # Store raw predictions for later ensemble\n",
        "                    fold_val_preds.extend(outputs.cpu().numpy())\n",
        "\n",
        "                    # Convert outputs to binary predictions\n",
        "                    preds = (outputs > 0.5).float()\n",
        "                    all_preds.extend(preds.cpu().numpy())\n",
        "                    all_targets.extend(targets.cpu().numpy())\n",
        "\n",
        "            # Store predictions for this fold\n",
        "            val_predictions = np.array(fold_val_preds)\n",
        "            val_fold_predictions[val_idx, fold] = val_predictions\n",
        "\n",
        "            # Calculate metrics\n",
        "            accuracy = accuracy_score(all_targets, all_preds)\n",
        "            f1 = f1_score(all_targets, all_preds, zero_division=1)\n",
        "\n",
        "            # Print progress\n",
        "            if (epoch + 1) % 10 == 0:\n",
        "                print(f\"Epoch {epoch+1}/{EPOCHS}, Train Loss: {train_loss/len(train_loader):.4f}, \"\n",
        "                      f\"Val Loss: {val_loss/len(val_loader):.4f}, Accuracy: {accuracy:.4f}, F1: {f1:.4f}\")\n",
        "\n",
        "            # Early stopping\n",
        "            if val_loss < best_val_loss:\n",
        "                best_val_loss = val_loss\n",
        "                early_stop_counter = 0\n",
        "                torch.save(model.state_dict(), f'best_model_fold{fold+1}.pt')\n",
        "            else:\n",
        "                early_stop_counter += 1\n",
        "                if early_stop_counter >= early_stop_patience:\n",
        "                    print(f\"Early stopping at epoch {epoch+1}\")\n",
        "                    break\n",
        "\n",
        "        # Load best model for evaluation\n",
        "        model.load_state_dict(torch.load(f'best_model_fold{fold+1}.pt'))\n",
        "        model.eval()\n",
        "\n",
        "        # Final evaluation on validation set\n",
        "        all_preds = []\n",
        "        all_targets = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for inputs, targets in val_loader:\n",
        "                inputs, targets = inputs.to(device), targets.to(device)\n",
        "                outputs = model(inputs)\n",
        "                preds = (outputs > 0.5).float()\n",
        "                all_preds.extend(preds.cpu().numpy())\n",
        "                all_targets.extend(targets.cpu().numpy())\n",
        "\n",
        "        # Calculate metrics\n",
        "        accuracy = accuracy_score(all_targets, all_preds)\n",
        "        f1 = f1_score(all_targets, all_preds, zero_division=1)\n",
        "        conf_matrix = confusion_matrix(all_targets, all_preds)\n",
        "\n",
        "        print(f\"\\nFold {fold+1} Results:\")\n",
        "        print(f\"Accuracy: {accuracy:.4f}\")\n",
        "        print(f\"F1 Score: {f1:.4f}\")\n",
        "        print(f\"Confusion Matrix:\\n{conf_matrix}\")\n",
        "\n",
        "        fold_results.append({\n",
        "            'fold': fold + 1,\n",
        "            'accuracy': accuracy,\n",
        "            'f1': f1\n",
        "        })\n",
        "\n",
        "    # Compute OOF predictions\n",
        "    oof_preds = np.zeros(len(X_scaled))\n",
        "    for fold, val_idx in enumerate(oof_indices):\n",
        "        oof_preds[val_idx] = val_fold_predictions[val_idx, fold]\n",
        "\n",
        "    # Convert to binary predictions\n",
        "    oof_preds_binary = (oof_preds > 0.5).astype(float)\n",
        "\n",
        "    # Calculate OOF metrics\n",
        "    oof_accuracy = accuracy_score(y, oof_preds_binary)\n",
        "    oof_f1 = f1_score(y, oof_preds_binary, zero_division=1)\n",
        "\n",
        "    # Print overall results\n",
        "    accuracies = [result['accuracy'] for result in fold_results]\n",
        "    f1_scores = [result['f1'] for result in fold_results]\n",
        "\n",
        "    print(\"\\nOverall Results:\")\n",
        "    print(f\"Mean CV Accuracy: {np.mean(accuracies):.4f} ± {np.std(accuracies):.4f}\")\n",
        "    print(f\"Mean CV F1 Score: {np.mean(f1_scores):.4f} ± {np.std(f1_scores):.4f}\")\n",
        "    print(f\"OOF Accuracy: {oof_accuracy:.4f}\")\n",
        "    print(f\"OOF F1 Score: {oof_f1:.4f}\")\n",
        "\n",
        "    # Create ensemble of best models\n",
        "    print(\"\\nCreating ensemble of models...\")\n",
        "    ensemble_preds = np.zeros(len(y))\n",
        "\n",
        "    # Use the entire dataset for final prediction\n",
        "    full_dataset = EEGDataset(torch.FloatTensor(X_scaled), torch.FloatTensor(y))\n",
        "    full_loader = DataLoader(full_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "    for fold in range(n_splits):\n",
        "        # Load model\n",
        "        model = SimpleEEGModel(input_features=X_scaled.shape[1]).to(device)\n",
        "        model.load_state_dict(torch.load(f'best_model_fold{fold+1}.pt'))\n",
        "        model.eval()\n",
        "\n",
        "        # Get predictions\n",
        "        fold_preds = []\n",
        "        with torch.no_grad():\n",
        "            for inputs, _ in full_loader:\n",
        "                inputs = inputs.to(device)\n",
        "                outputs = model(inputs)\n",
        "                preds = outputs.cpu().numpy()\n",
        "                fold_preds.extend(preds)\n",
        "\n",
        "        # Add to ensemble predictions\n",
        "        ensemble_preds += np.array(fold_preds)\n",
        "\n",
        "    # Average predictions and convert to binary\n",
        "    ensemble_preds /= n_splits\n",
        "    ensemble_preds_binary = (ensemble_preds > 0.5).astype(float)\n",
        "\n",
        "    # Calculate ensemble metrics\n",
        "    ensemble_accuracy = accuracy_score(y, ensemble_preds_binary)\n",
        "    ensemble_f1 = f1_score(y, ensemble_preds_binary, zero_division=1)\n",
        "    ensemble_conf_matrix = confusion_matrix(y, ensemble_preds_binary)\n",
        "\n",
        "    print(\"\\nEnsemble Model Results:\")\n",
        "    print(f\"Accuracy: {ensemble_accuracy:.4f}\")\n",
        "    print(f\"F1 Score: {ensemble_f1:.4f}\")\n",
        "    print(f\"Confusion Matrix:\\n{ensemble_conf_matrix}\")\n",
        "\n",
        "    return ensemble_accuracy, ensemble_f1\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    start_time = time.time()\n",
        "    acc, f1 = train_and_evaluate()\n",
        "    end_time = time.time()\n",
        "\n",
        "    print(f\"\\nTotal execution time: {(end_time - start_time) / 60:.2f} minutes\")\n",
        "    print(f\"Final Accuracy: {acc:.4f}\")\n",
        "    print(f\"Final F1 Score: {f1:.4f}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
